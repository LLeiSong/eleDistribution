---
title: 'Part1: Landscape priority modeling at coarse level'
author: "Lei Song"
date: '2022-07-12'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

## Introduction

Due to the limitation of available occurrence datasets, and also make the most of the existing valuable datasets (e.g. elephant census), we split the whole modeling process into a nested multi-scale structure, which mainly include two parts. Within part1, African elephants' preference of landscape at broad scale will be modeled. Expert range map and the elephant census dataset are used together to generate pseudo-occurrences for modeling. Because this part will do modeling in large scale, the distance-based features cannot perform well. So we use density-based features. These feature include roads, settlement, and rivers. Environmental variables include:

**Climate:**

- Selected bioclimatic variables (check)

**Vegetation:**

- Mean NDVI during wet season (check)
- Mean NDVI during dry season (check)
- NDVI Seasonality (standard deviation ×100) (check)

**Landscape structure:**

- Selected landscape metrics at landscape level (check)
- Selected landscape metrics at class level (check)
- Ratio of selected land cover types (check)
- Rivers/streams density (check)

**Anthropological impacts:**

- Settlement density (check)
- Road density (check)
- Cropland density

**Topographic features:**

- Terrain roughness (check)
- Slope (check)

## Elephant datasets
### Range map

The datasets are inquired from [elephant database](https://africanelephantdatabase.org). People want to use these datasets should inquired directly from them.

```{r}
library(sf)
library(here)
library(raster)
library(terra)
library(ggplot2)
library(purrr)
library(pbmcapply)
library(dplyr)

data_path <- here("data")
rst_path <- file.path(data_path, "rasters")
vct_path <- file.path(data_path, "vectors")
hr_path <- file.path(data_path, "vars_home_range")
result_path <- here("results")
walk(c(rst_path, vct_path, hr_path, result_path), function(pth) {
    if (!dir.exists(pth)) dir.create(pth)
})

# Get the range map
range_map <- read_sf(
    file.path(data_path, "observations/elephant_database",
              "Range_Layer_bush/AED_Range_Layer_2015.shp")) %>% 
    filter(country == "Tanzania") %>%
    filter(range == "Known") %>% 
    select(range, area_sqkm)

# Check it
ggplot() + 
    geom_sf(data = range_map, aes(fill = range)) +
    scale_fill_brewer("Extant", palette = "Dark2") +
    theme_classic()

# Save out
write_sf(range_map, file.path(data_path, "observations/range_map.geojson"))
```

### Census

The census data is inquired from [elephant database](https://africanelephantdatabase.org) as well.

```{r}
# Read the data
census <- read_sf(
    file.path(data_path, 
              "observations/elephant_database/Input_Zones_Layer",
              "AED_Input_Zones_2016_Data_Sharing.shp")) %>% 
    filter(country == "Tanzania") %>% 
    # remove the too old ones
    filter(year > 2010) %>% 
    select(inpzone, stratum, estimate, area_sqkm)
census$estimate <- as.numeric(census$estimate)
census$area_sqkm <- as.numeric(census$area_sqkm)

# Calculate the population density
census <- census %>% mutate(density = estimate / area_sqkm)

# Check it
ggplot() + 
    geom_sf(data = census, aes(fill = density)) +
    scale_fill_viridis_c("Density") +
    theme_classic()

# Save out
write_sf(census, file.path(data_path, "observations/census.geojson"))
```

### Put them together

We will set weights to the pseudo occurrence samples generated by range and census dataset. Then how to set the weights? In census, we used population density as the weights. And use the average population density as the weight for range map where is not covered by census blocks. The area with population density 0 will be removed.

```{r}
# Assign the min population density
range_map <- st_difference(range_map, st_union(census))
range_map <- range_map %>% st_as_sf() %>% 
    select() %>% 
    mutate(area_sqkm = st_area(.) %>% units::set_units('km2'),
           #density = min(census$density[census$density > 0])) %>% 
           density = mean(census$density)) %>% 
    mutate(estimate = as.integer(ceiling(density * area_sqkm)))

# Put together
pseudo_blocks <- rbind(range_map %>% select(estimate, density),
                       census %>% select(estimate, density))

# Check it
ggplot() + 
    geom_sf(data = pseudo_blocks, aes(fill = density)) +
    scale_fill_viridis_c("Density") +
    theme_classic()

# Save out
write_sf(pseudo_blocks, 
         file.path(data_path, "observations/pseudo_blocks.geojson"))
```

### Occurrence of African savanna elephant

We tried to find any public occurrence datasets to use. So far, GBIF occurrence dataset and a publicly available dataset related to genotype can be used as occurrence. In part1, the aggregated occurrence will be used as independent test dataset.

#### GBIF

Query the occurrence data using GBIF API. To reduce the impacts of landscape change, we only selected the occurrence after year 2015 to use. Then we cleaned the occurrence for any possible errors. 

```{r}
# remotes::install_github("ropensci/scrubr")
library(scrubr, quietly = TRUE)
library(lubridate, quietly = TRUE)
library(rgbif, quietly = TRUE)

## Set the time interval for querying on GBIF
start_year <- 2015
year <- sprintf('%s,%s',  start_year, year(Sys.Date()))

# Search
nm_search <- "Loxodonta africana (Blumenbach, 1797)"
occ <- occ_search(scientificName = nm_search,
                  country = "TZ",
                  hasCoordinate = TRUE,
                  limit = 200000,
                  year = year,
                  hasGeospatialIssue = FALSE)

# Clean the dataset
occ <- dframe(occ$data) %>%
    coord_impossible() %>%
    coord_incomplete() %>%
    coord_unlikely()

# Convert to sf
occ <- st_as_sf(occ[, c("decimalLongitude", "decimalLatitude")], 
                coords = c("decimalLongitude", "decimalLatitude"),
                crs = 4326)
rm(start_year, year, nm_search)
```

#### Genotype dataset

This dataset is shared by the paper *Genetic connectivity and population structure of African savanna elephants (Loxodonta africana) in Tanzania ([https://doi.org/10.1002/ece3.6728](https://doi.org/10.1002/ece3.6728))*. We converted the records to spatial points to use.

```{r}
fname <- file.path(data_path, "observations/genetype_dataset_15_17.csv")
occ2 <- read.csv(fname, stringsAsFactors = FALSE)
occ2 <- st_as_sf(occ2[, c("Lat", "Long")], 
                coords = c("Long", "Lat"),
                crs = 4326)
rm(fname)
```

#### Combine two datasets and clean

Combine two datasets and clip to the national boundary.

```{r}
occ <- rbind(occ, occ2); rm(occ2)
bry <- read_sf(file.path(data_path, "vectors/mainland_tanzania.geojson"))
occ <- occ[unique(unlist(st_intersects(bry, occ))), ]

# Save out and clean
write_sf(occ, file.path(data_path, "observations/ele_occurrence.geojson"))
rm(bry, occ)
```

## Environmental variables

Because of the limitation of range map/census map, we need to find out the most effective scale to use them. Thus, we tested scale 2.5 minutes, 5 minutes, 10 minutes and 20 minutes.

### Climate

Download Bioclimatic variables and check the correlation. According to literature, expert knowledge and data itself, we selected:

- BIO1 = Annual Mean Temperature
- BIO4 = Temperature Seasonality (standard deviation ×100)
- BIO7 = Temperature Annual Range (BIO5-BIO6)
- BIO12 = Annual Precipitation
- BIO15 = Precipitation Seasonality (Coefficient of Variation)

```{r}
library(itsdm)
library(stars)
library(purrr)
library(corrplot)

# Make a place to store data
cli_path <- file.path(data_path, "worldclim")
dir.create(cli_path)

# Download worldclim Bioclimatic variables.
sf_use_s2(FALSE)
bry <- read_sf(file.path(data_path, "vectors/mainland_tanzania.geojson"))
bry <- st_buffer(bry, 0.5)
walk(c(2.5, 5, 10), function(scale) {
    bios <- worldclim2(var = "bio", res = scale,
                   bry = bry, path = tempdir())
    write_stars(bios, 
                file.path(cli_path, sprintf("bios_%s.tif", scale)))
}); rm(bios)

# Analyze the correlation
fnames <- list.files(cli_path, full.names = TRUE)
walk(fnames, function(fname) {
    bios <- rast(fname)
    bios <- values(bios)
    bios <- data.frame(bios) %>% na.omit()
    corrplot(corr = cor(bios, method = "spearman"), 
             method = "square", type = "lower",
             diag = FALSE, addCoef.col = "black", 
             number.cex = 0.7, tl.cex = 0.7)
})
# The above figures show massive correlation between bio variables

# Subset our selected variables
walk(fnames, function(fname) {
    bios <- rast(fname)
    bios <- values(bios)
    bios <- data.frame(bios) %>% na.omit()
    bios <- bios %>% select(paste0("bio", c(1, 4, 7, 12, 15)))
    corrplot(corr = cor(bios, method = "spearman"), 
             method = "square", type = "lower",
             diag = FALSE, addCoef.col = "black", 
             number.cex = 0.7, tl.cex = 0.7)
})
# The figure above show healthy correlation between selected variables

# Save out
bry <- read_sf(file.path(data_path, "vectors/mainland_tanzania.geojson"))
bry <- vect(bry)
walk(fnames, function(fname) {
    bios <- rast(fname)
    bios <- subset(bios, c(1, 4, 7, 12, 15))
    bios <- mask(bios, bry, touches = TRUE)
    writeRaster(bios, file.path(hr_path, basename(fname)))
})
```

### Vegetation

- Mean NDVI during wet season
- Mean NDVI during dry season
- NDVI Seasonality (standard deviation)

We made the calculation in Google Earth Engine to take the advantage of it. Export from GEE, read the images, and resample to the dimension of bios.

```{r}
# Dry season
ndvi_dry <- rast(file.path(data_path, 
                           "NDVI/lansat8_sr_13_19_dry_NDVI_mean_1000m.tif"))
# In case there are any missing values
ndvi_dry <- focal(ndvi_dry, fun = mean, na.rm = TRUE, na.policy = "only")

# Wet season
ndvi_wet <- rast(file.path(data_path, 
                           "NDVI/lansat8_sr_13_19_wet_NDVI_mean_1000m.tif"))
ndvi_wet <- focal(ndvi_wet, fun = mean, na.rm = TRUE, na.policy = "only")

# Mean annual SD
ndvi_sd <- rast(file.path(data_path, 
                           "NDVI/lansat8_sr_13_19_NDVI_mean_sd_1000m.tif"))
ndvi_sd <- focal(ndvi_sd, fun = mean, na.rm = TRUE, na.policy = "only")
ndvis <- c(ndvi_dry, ndvi_wet, ndvi_sd)
names(ndvis) <- c("ndvi_dry_season", "ndvi_wet_season", "ndvi_seasonality")

writeRaster(ndvis, file.path(rst_path, "ndvi.tif"))

# Upscale
ndvis <- stack(ndvis)
walk(c(10, 5, 2.5), function(scale) {
    message(sprintf("Process scale: %s", scale))
    template <- raster(file.path(hr_path, sprintf("bios_%s.tif", scale)), 
                     band = 1)
    values(template) <- 1:ncell(template)
    vals <- do.call(rbind, pbmclapply(1:ncell(template), function(id) {
        blk <- classify(rast(template), cbind(id, 1), others = NA)
        blk <- trim(blk)
        
        # Do calculation for each window size
        if (relate(as.polygons(ext(ndvis)), 
                   as.polygons(ext(blk)), "intersects")) {
            # Crop the landscape
            ndvi_blk <- crop(rast(ndvis), blk)
            
            # Calculate the metrics
            if (isTRUE(global(ndvi_blk[[1]], fun = "isNA") != ncell(ndvi_blk))) {
                values(ndvi_blk) %>% colMeans() %>% rbind() %>% data.frame()
            } else {
                return(NA)
            }
        } else return(NA)
    }, mc.cores = detectCores() - 1))
    
    # Burn values to raster
    ndvi_out <- rep(rast(template), ncol(vals))
    values(ndvi_out) <- as.matrix(vals)
    names(ndvi_out) <- names(vals)
    dst_path <- file.path(hr_path, sprintf("ndvi_%s.tif", scale))
    writeRaster(ndvi_out, dst_path)
})
```

### Landscape structure
#### Landscape metrics

**Landscape level:**

- Aggregation index (AI)
- Patch density (PD)
- Marginal entropy (ENT)
- Conditional entropy
- Joint entropy
- Mutual information
- Edge density (ED)
- Patch richness density (PRD)
- Simpson’s diversity index (SIDI)
- Shannon’s diversity index (SHDI)

**Class level:**

Dense tree/cropland

- class ratio
- Edge density (ED)

Savanna (shrub + grass)

- Largest patch index (LPI)
- Patch density (PD)
- Contiguity index distribution (mean) (CONTIG_MN)

**Ratio of land cover types:**

- Cropland (one of anthropological impacts)
- Dense tree/forest
- Savanna (Shrub, grassland, and wetland)
- Water

```{r}
source(file.path(here("scripts"), "landscape_metrics.R"))

# Read datasets
lc <- rast(file.path(data_path, "landcover/landcover.tif"))
# Aggregate class types
# 1, cropland, 2, dense tree, 3, savanna (shrub + grass + wetland), 4, water
lc_class <- classify(lc, rbind(c(1, 1), c(2, 2),
                               c(3, 3), c(4, 3),
                               c(5, 4), c(8, 3)), others = NA)
writeRaster(lc_class, 
            file.path(data_path, "landcover/crop_tree_savanna_water.tif"),
            datatype = "INT1U")

# Landscape level
walk(c(10, 5, 2.5), function(scale) {
    message(sprintf("Process scale: %s", scale))
    template <- rast(file.path(hr_path, sprintf("bios_%s.tif", scale)), 
                  lyrs = 1)
    values(template) <- 1:ncell(template)
    dst_path <- file.path(hr_path, sprintf("lsp_l_metrics_%s.tif", scale))
    landscape_l_metrics(lc, template, dst_path)
}); rm(lc)

# Class level
lc_class <- rast(file.path(data_path, "landcover/crop_tree_savanna_water.tif"))
walk(c(10, 5, 2.5), function(scale) {
    message(sprintf("Process scale: %s", scale))
    template <- rast(file.path(hr_path, sprintf("bios_%s.tif", scale)), 
                     lyrs = 1)
    values(template) <- 1:ncell(template)
    dst_path <- file.path(hr_path, sprintf("lsp_c_metrics_%s.tif", scale))
    landscape_c_metrics(lc_class, template, dst_path)
}); rm(lc_class)
```

### Anthropological impacts and density features

- Settlement density
- Road density
- Cropland density (calculated together with landscape metrics)

The raw data is downloaded from [https://download.geofabrik.de/africa.html](https://download.geofabrik.de/africa.html).

#### Waterbodies, rivers, and streams

Waterbbodies and deep rivers should act as barriers for animal movement.

```{r}
library(sf)

# Waterbodies
fname <- list.files(file.path(data_path, "osm"),
                    pattern = "osm_water_a_free_1.shp",
                    full.names = TRUE)
waterbodies <- read_sf(fname)
waterbodies <- waterbodies[
    !waterbodies$fclass %in% c("wetland", "glacier", "dock"), ]

# Rivers
fname <- list.files(file.path(data_path, "osm"),
                    pattern = "osm_waterways_free_1.shp",
                    full.names = TRUE)
rivers <- read_sf(fname)
streams <- rivers[rivers$fclass == "stream", ]
rivers <- rivers[rivers$fclass == "river", ]

# Save out
write_sf(waterbodies, file.path(vct_path, "waterbodies.geojson"))
write_sf(rivers, file.path(vct_path, "rivers.geojson"))
write_sf(streams, file.path(vct_path, "streams.geojson"))

# Remove temporary objects
rm(waterbodies, rivers, streams); gc()
```

#### Roads/railways

```{r}
# Railways
fname <- list.files(file.path(data_path, "osm"),
                    pattern = "osm_railways_free_1.shp",
                    full.names = TRUE)
railways <- read_sf(fname)

# Roads
fname <- list.files(file.path(data_path, "osm"),
                    pattern = "osm_roads_free_1.shp",
                    full.names = TRUE)
roads <- read_sf(fname)

## Remove residential roads
roads <- roads[
    !roads$fclass %in% c("residential", "service", "steps",
                         "pedestrian", "path", "footway", 
                         "cycleway", "living_street"), ]

## Big roads
big_roads <- roads[
    roads$fclass %in% c("trunk", "primary", "trunk_link", "primary_link"), ]
big_roads <- rbind(railways[, c("osm_id", "fclass")], 
                   big_roads[, c("osm_id", "fclass")])

# Save out
write_sf(big_roads, file.path(vct_path, "big_roads.geojson"))
write_sf(roads, file.path(vct_path, "roads.geojson"))

# Remove temporary objects
rm(railways, roads, big_roads); gc()
```

#### Distances

This part is just for the record. We will not use distance-based features for coarse scale analysis. We upscale landcover using area preserving method first to keep the reasonable landscape structure before calculate the distance to cropland.

```{r}
# devtools::install_github("LLeiSong/APUpscale")
library(APUpscale)
library(purrr)

landscape <- rast(file.path(data_path, "landcover/landcover.tif"))
landscape <- upscale(landscape, 
                     cellsize = 1000, 
                     verbose = verbose)
writeRaster(landscape, file.path(data_path, "landcover/landcover_1000m.tif"),
            overwrite = T,
            wopt = list(datatype = 'INT1U',
                        gdal=c("COMPRESS=LZW")))

# Calculate the distances
source(file.path(here("scripts"), "distances.R"))

# Distances
walk(c(10, 5, 2.5), function(scale) {
    message(sprintf("Calculate distance in scale: %s", scale))
    distances(data_path, vct_path, hr_path, scale)
})
```

#### Density
##### Settlement density

Here, we use [Google Open Buildings](https://sites.research.google/open-buildings/) for a better quality from [here](https://sites.research.google/open-buildings/#download).

```{r}
library(sf)
library(terra)
library(pbmcapply)

# Boundary
bry <- read_sf(file.path(vct_path, "mainland_tanzania.geojson"))
bry <- vect(bry["FID"])

# Buildings
fname <- list.files(file.path(data_path, "gob"), full.names = TRUE)

buildings <- read.csv(fname, stringsAsFactors = FALSE)
buildings <- buildings %>% 
  dplyr::select(geometry) %>% 
  dplyr::mutate(geometry = st_as_sfc(geometry)) %>% 
  st_as_sf() %>% st_set_crs(4326) %>% 
  st_make_valid()
buildings <- buildings[!st_is_empty(buildings), , drop = FALSE]

# Save out
write_sf(buildings, file.path(vct_path, "buildings.geojson"))

# calculate the pixel-wise count
# Read buildings and pack it for parallel;
# buildings <- read_sf(file.path(vct_path, "buildings.geojson"))
buildings <- vect(buildings)
buildings <- centroids(buildings, inside = FALSE)
buildings <- wrap(buildings)

# Cut the whole image to tiles
# I guess large vector can slow down the computation significantly
template <- rast(file.path(hr_path, "bios_2.5.tif"),
                 lyrs = 1)
values(template) <- NA
parts <- aggregate(template, fact = 60)
temp_dir <- tempdir()
output_tiles <- makeTiles(
    template, parts,
    filename = file.path(temp_dir, 'output_.tif'))
rm(temp_dir, parts, template)

# Summarize the settlement count within each cell tile by tile
temp_dir <- file.path(tempdir(), "results_settlement")
dir.create(temp_dir)
message('Save raster chunks to %s', temp_dir)

settlement_density <- pbmclapply(output_tiles, function(fname) {
    outname <- file.path(temp_dir, basename(fname))
    rst <- rast(fname)
    vct <- crop(vect(buildings), rst)
    if (nrow(vct) == 0) {
        values(rst) <- 0
        writeRaster(rst, outname)
    } else {
        rst <- rasterizeGeom(vct, rst, fun = "count")
        writeRaster(rst, outname)
    }
    rm(rst, vct); gc()
    outname
}, mc.cores = 6, ignore.interactive = TRUE)
settlement_density <- vrt(unlist(settlement_density))
writeRaster(settlement_density, 
            file.path(hr_path, "settlement_density_2.5.tif"))
rm(buildings, settlement_density)
# unlink(temp_dir, recursive = TRUE)
gc()

# Upscale
walk(c(5, 10), function(scale) {
    template <- rast(file.path(hr_path, sprintf("bios_%s.tif", scale)),
                 lyrs = 1)
    values(template) <- 1:ncell(template)
    zones <- resample(template, settlement_density, method = "near")
    settlement_density_us <- zonal(settlement_density, zones, 
                                   fun = sum)
    values(template) <- settlement_density_us[, 2]
    writeRaster(template, 
                file.path(hr_path, 
                          sprintf("settlement_density_%s.tif", scale)))
})
```

##### Roads density

```{r}
# Read roads and pack it for parallel
roads <- vect(file.path(vct_path, "roads.geojson"))
roads <- roads[, -c(1:ncol(roads))]
roads <- wrap(roads)

# Summarize the road length within each cell tile by tile
temp_dir <- file.path(tempdir(), "results_roads")
dir.create(temp_dir)
message('Save raster chunks to %s', temp_dir)

roads_density <- pbmclapply(output_tiles, function(fname) {
    outname <- file.path(temp_dir, basename(fname))
    rst <- rast(fname)
    vct <- crop(vect(roads), rst)
    if (nrow(vct) == 0) {
        values(rst) <- 0
        writeRaster(rst, outname)
    } else {
        rst <- rasterizeGeom(vct, rst, fun = "length", unit = "km")
        writeRaster(rst, outname)
    }
    outname
}, mc.cores = 6, ignore.interactive = TRUE)

roads_density <- vrt(unlist(roads_density))
writeRaster(roads_density, file.path(hr_path, 
                                     "roads_density_2.5.tif"))
rm(roads, roads_density)
# unlink(temp_dir, recursive = TRUE)
gc()

# upscale
walk(c(5, 10), function(scale) {
    template <- rast(file.path(hr_path, sprintf("bios_%s.tif", scale)),
                 lyrs = 1)
    values(template) <- 1:ncell(template)
    zones <- resample(template, roads_density, method = "near")
    roads_density_us <- zonal(roads_density, zones, fun = sum)
    values(template) <- roads_density_us[, 2]
    writeRaster(template, 
                file.path(hr_path, 
                          sprintf("roads_density_%s.tif", scale)))
})
```

##### River density

```{r}
# Read rivers and streams and pack it for parallel
rivers <- rbind(vect(file.path(vct_path, "rivers.geojson")),
                vect(file.path(vct_path, "streams.geojson")))
rivers <- rivers[, -c(1:ncol(rivers))]
rivers <- wrap(rivers)

# Summarize the river length within each cell tile by tile
temp_dir <- file.path(tempdir(), "results_rivers")
dir.create(temp_dir)
message('Save raster chunks to %s', temp_dir)

rivers_density <- pbmclapply(output_tiles, function(fname) {
    outname <- file.path(temp_dir, basename(fname))
    rst <- rast(fname)
    vct <- crop(vect(rivers), rst)
    if (nrow(vct) == 0) {
        values(rst) <- 0
        writeRaster(rst, outname)
    } else {
        rst <- rasterizeGeom(vct, rst, fun = "length", unit = "km")
        writeRaster(rst, outname)
    }
    outname
}, mc.cores = 6, ignore.interactive = TRUE)

rivers_density <- vrt(unlist(rivers_density))
writeRaster(rivers_density, file.path(hr_path, 
                                      "rivers_density_2.5.tif"))
rm(rivers, rivers_density)
unlink(temp_dir, recursive = TRUE)

# Remove temporary files
file.remove(output_tiles)
rm(rivers, rivers_density, output_tiles, temp_dir)

# upscale
walk(c(5, 10), function(scale) {
    template <- rast(file.path(hr_path, sprintf("bios_%s.tif", scale)),
                 lyrs = 1)
    values(template) <- 1:ncell(template)
    zones <- resample(template, rivers_density, method = "near")
    rivers_density_us <- zonal(rivers_density, zones, fun = sum)
    values(template) <- rivers_density_us[, 2]
    writeRaster(template, 
                file.path(hr_path, 
                          sprintf("rivers_density_%s.tif", scale)))
})
```

### Topographic features

- Terrain roughness (SD of elevation within a cell)
- Slope (SD of slope within a cell)

We used Advanced Land Observing Satellite (ALOS) DSM. DSM is the alternative elevation product of SRTM with approximately 30 meters resolution. 

##### Bounding range of Tanzania

```{r}
# Get the range
bry <- read_sf(file.path(data_path, "vectors/mainland_tanzania.geojson"))
bry <- bry[, "FID"]
ranges <- st_bbox(bry)
ranges[1:2] <- floor(ranges[1:2])
ranges[3:4] <- ceiling(ranges[3:4])
```

##### Target the DSM tiles and download

```{r}
library(stringr)

# Get the tiles
## longitude
if (ranges[1] > 0 & ranges[3] >= 0) {
    message("The whole area is in East side.")
    
    start_loc <- ranges[1] %/% 5 * 5
    end_loc <- (ranges[3] %/% 5 + 1) * 5
    lon_zones <- seq(start_loc, end_loc, 5)
    lon_zones <- paste0("E", str_pad(lon_zones, 3, pad = "0"))
} else if (ranges[1] < 0 & ranges[3] >= 0) {
    message("The whole area is across West and East side.")
    
    start_loc <- ranges[1] %/% 5 * 5
    end_loc <- (ranges[3] %/% 5 + 1) * 5
    lon_zones <- seq(start_loc, end_loc, 5)
    lon_signs <- ifelse(lon_zones >= 0, "E", "W")
    lon_zones <- paste0(lon_signs, str_pad(abs(lon_zones), 3, pad = "0"))
    rm(lon_signs)
} else if (ranges[1] < 0 & ranges[3] < 0) {
    message("The whole area is in West side.")
    
    start_loc <- ranges[1] %/% 5 * 5
    end_loc <- (ranges[3] %/% 5 + 1) * 5
    lon_zones <- seq(start_loc, end_loc, 5)
    lon_zones <- paste0("W", str_pad(abs(lon_zones), 3, pad = "0"))
}

## latitude
if (ranges[2] > 0 & ranges[4] >= 0) {
    message("The whole area is in North side.")
    
    start_loc <- ranges[2] %/% 5 * 5
    end_loc <- (ranges[4] %/% 5 + 1) * 5
    lat_zones <- seq(start_loc, end_loc, 5)
    lat_zones <- paste0("N", str_pad(lat_zones, 3, pad = "0"))
} else if (ranges[2] < 0 & ranges[4] >= 0) {
    message("The whole area is across South and North side.")
    
    start_loc <- ranges[2] %/% 5 * 5
    end_loc <- (ranges[4] %/% 5 + 1) * 5
    lat_zones <- seq(start_loc, end_loc, 5)
    lat_signs <- ifelse(lat_zones >= 0, "N", "S")
    lat_zones <- paste0(lat_signs, str_pad(abs(lat_zones), 3, pad = "0"))
    rm(lat_signs)
} else if (ranges[2] < 0 & ranges[4] < 0) {
    message("The whole area is in South side.")
    
    start_loc <- ranges[2] %/% 5 * 5
    end_loc <- (ranges[4] %/% 5 + 1) * 5
    lat_zones <- seq(start_loc, end_loc, 5)
    lat_zones <- paste0("S", str_pad(abs(lat_zones), 3, pad = "0"))
}; rm(ranges, start_loc, end_loc)

zones <- sapply(1:(length(lat_zones) - 1), function(n) {
    sprintf("%s%s_%s%s", lat_zones[n], 
            lon_zones[1:(length(lon_zones) - 1)],
            lat_zones[n + 1], 
            lon_zones[2:length(lon_zones)])})
base_url <- "https://www.eorc.jaxa.jp/ALOS/aw3d30/data/release_v2012"
urls <- file.path(base_url, sprintf("%s.zip", zones))
rm(zones, base_url, lat_zones, lon_zones)

dst_path <- file.path(data_path, "DSM")
if (!dir.exists(dst_path)) dir.create(dst_path)

opts <- options()
options(timeout = 60 * 120)
lapply(urls, function (url_dl) {
    message(sprintf("Download %s to %s", url_dl, dst_path))
    
    # Download
    fname <- file.path(dst_path, basename(url_dl))
    tryCatch({
        download.file(url_dl, 
                  destfile = fname)
    }, error = function(e) {
        warning("Failed to download, try again.")
        file.remove(fname)
        tryCatch({
            download.file(url_dl, 
                  destfile = fname)
        }, error = function(e) {
            warning("Failed to download again.")
            file.remove(fname)})})
    
    # unzip
    if (file.exists(fname)) {
        cmd <- sprintf("unzip %s -d %s", fname, dst_path)
        try(system(cmd, intern = TRUE, ignore.stdout = TRUE))
        file.remove(fname)}
}); options(opts)
rm(bopts, cmd, data_path, fname, url_dl, urls)
```

##### Mosaic to a single file

```{r}
# Mosaic them together
dsm <- do.call(merge, lapply(
    list.files(dst_path, recursive = TRUE, 
               pattern = "DSM.tif", 
               full.names = TRUE), function(fname) {
                   rast(fname)}))
dsm <- crop(dsm, bry)
writeRaster(dsm, file.path(rst_path, "dsm_tanzania.tif"),
            datatype = "INT4S", overwrite = TRUE,
            wopt = list(gdal=c("COMPRESS=LZW")))
```

#### Topographic roughness

Here, we calculated standard deviation of elevation and slope within each BIO cell.

```{r}
library(raster)
library(purrr)
library(pbmcapply)

dsm <- raster(dsm)
walk(c(10, 5, 2.5), function(scale) {
    message(sprintf("Process scale: %s", scale))
    template <- raster(file.path(hr_path, sprintf("bios_%s.tif", scale)), 
                     band = 1)
    values(template) <- 1:ncell(template)
    vals <- do.call(rbind, pbmclapply(1:ncell(template), function(id) {
        blk <- classify(rast(template), cbind(id, 1), others = NA)
        blk <- trim(blk)
        
        # Do calculation for each window size
        if (relate(as.polygons(ext(dsm)), 
                   as.polygons(ext(blk)), "intersects")) {
            # Crop the landscape
            dsm_blk <- crop(rast(dsm), blk)
            
            # Calculate the metrics
            if (isTRUE(global(dsm_blk, fun = "isNA") != ncell(dsm_blk))) {
                sd_elev <- sd(values(dsm_blk), na.rm = TRUE)
                sd_slope <- sd(values(terrain(dsm_blk, "slope")), na.rm = TRUE)
                data.frame(sd_elevation = sd_elev, sd_slope = sd_slope)
            } else {
                return(NA)
            }
        } else return(NA)
    }, mc.cores = detectCores() - 1))
    
    # Burn values to raster
    roughness <- rep(rast(template), ncol(vals))
    values(roughness) <- as.matrix(vals)
    names(roughness) <- names(vals)
    dst_path <- file.path(hr_path, sprintf("terrain_roughness_%s.tif", scale))
    writeRaster(roughness, dst_path)
})
```

## Modeling in different scales

- 2.5 minute
- 5 minute
- 10 minute

Here, we use Isolation forest method for modeling in this part. Like a regular modeling, we do parameter tuning, select the best model, and run the final model. Differently, we used random sampling within range map as pseudo occurrences for modeling. And we use the average of 10-folds for the final model. We use the true occurrence as an "independent test" to select the best scale.

### Gather data

In this step, we stack all environmental variables for each scale, and fill the potential NAs.

```{r}
source(file.path(here("scripts"), "gather_data.R"))

# boundary
bry <- read_sf(file.path(data_path, "vectors/mainland_tanzania.geojson"))
bry <- vect(bry[, 'FID'])

walk(c(2.5, 5, 10), function(scale) {
    gather_data_cs(bry, hr_path, scale)
})
```

### Variable analysis

Due to the high correlation between landscape metrics, we do pair-wise correlation check again. And remove the variables that are highly correlated with others. We keep some highly-correlated ones, such as dry season NDVI and wet season NDVI because we think both of them are ecological meaningful.

```{r}
# Check correlation and reduce dimension
walk(c(2.5, 5, 10), function(scale) {
    vars <- rast(file.path(hr_path, sprintf("variables_%s.tif", scale)))
    vars <- values(vars)
    vars <- data.frame(vars) %>% na.omit()
    vars <- vars %>% 
        select(-c(sd_slope, ai, sidi, condent, 
                  ent, joinent, mutinf, ed, savanna_lpi,
                  cropland_ratio, tree_ratio))
    corrplot(corr = cor(vars, method = "spearman"), 
             method = "square", type = "lower",
             diag = FALSE, addCoef.col = "black", 
             number.cex = 0.7, tl.cex = 0.7)
})

# Save out the selected variables
walk(c(2.5, 5, 10), function(scale) {
    vars <- rast(file.path(hr_path, sprintf("variables_%s.tif", scale)))
    bands_select <- setdiff(names(vars), 
                            c('sd_slope', 'ai', 'sidi', 'condent', 
                              'ent', 'joinent', 'mutinf', 'ed', 'savanna_lpi',
                              'cropland_ratio', 'tree_ratio'))
    vars <- subset(vars, bands_select)
    writeRaster(vars, 
                file.path(hr_path, 
                          sprintf("variables_final_%s.tif", scale)))
})
```

So the final variables to use are:

**Bioclimatic variables:**

- BIO1 = Annual Mean Temperature
- BIO4 = Temperature Seasonality (standard deviation ×100)
- BIO7 = Temperature Annual Range (BIO5-BIO6)
- BIO12 = Annual Precipitation
- BIO15 = Precipitation Seasonality (Coefficient of Variation)

**Vegetation:**

- Mean NDVI during wet season
- Mean NDVI during dry season
- NDVI Seasonality (standard deviation)

**Density-based features:**

- Settlement density
- Roads density
- River density

**Topography:**

- SD of elevation

**Landscape condition:**

Landscape level:

- Patch density (PD)
- Patch richness density (PRD)
- Shannon’s diversity index (SHDI)
- Ratio of Savanna (Shrub, grassland, and wetland)
- Ratio of Water

Class level:

- Edge density (ED) of cropland
- Edge density (ED) of tree
- Contiguity index distribution (mean) (CONTIG_MN) of savanna
- Patch density (PD) of savanna

### Modeling with Isolation Forest

Isolation forest works like an advanced version of bioclim. The direct result can be interpreted as environmental niche. So we use this method at coarse scale in this part even though the test performance may be slightly lower. This makes sense because the result is environmental niche rather than probability of occurrence and elephant is a common species in Tanzania.

#### Hyper-parameter tuning

Here, we first tune the model parameters according to others' suggestions:

- sample_size = c(0.8, 0.85, 0.9, 0.95)
- max_depth = seq(30, min(floor(num_sample / 2), 180), 30)
- ndim = c(2, 3, 4)
- scoring_metric = c("depth", "adj_depth")

```{r}
source(here("scripts/modeling.R"))

census_block <- read_sf(
    file.path(data_path, "observations/pseudo_blocks.geojson")) %>% 
    filter(density > 0)

walk(c(10, 5, 2.5), function(scale) {
    param_tuning_cs(census_block, scale, hr_path, result_path)
})
```

Figure

```{r}
library(tidyverse)
fg_path <- here("docs/figures")

evals <- do.call(rbind, lapply(c(10, 5, 2.5), function(scale){
    do.call(rbind,lapply(c(0.1, 0.2, 0.3, 0.4, 0.5), function (ratio) {
        load(file.path(result_path, 
                       sprintf("tuning_cv_%s_%s.rda", scale, ratio)))
        
        tuning_cv %>% 
            select(sample_size, max_depth, ndim, scoring_metric,
                   cbi, auc_ratio, TSS, auc, `f-measure`) %>% 
            mutate(ratio = ratio)
    })) %>% mutate(scale = scale)
})) %>% rename(CBI = cbi, `AUC (ratio)` = auc_ratio, 
               AUC = auc, `F-measure` = `f-measure`)

# Check and compare the results
evals <- evals %>%
    pivot_longer(cols = c(`AUC (ratio)`, TSS, AUC, `F-measure`),
                 names_to = "metrics", values_to = "value") %>% 
    mutate(scale = factor(scale, levels = c(2.5, 5, 10)),
           ratio = factor(ratio, levels = c(0.1, 0.2, 0.3, 0.4, 0.5)))

ggplot(evals, 
       aes(x = metrics, y = value, fill = ratio)) +
    geom_boxplot(outlier.colour = "white", outlier.shape = 8,
                 outlier.size = 2) +
    scale_fill_brewer("Sampling ratio", palette = "Dark2") +
    ylab('') +
    xlab('Evaluation metrics') +
    facet_wrap(~scale) +
    theme_classic() +
    theme(text = element_text(size = 14),
          axis.text.x = element_text(angle = 0),
          axis.text = element_text(size = rel(0.8)),
          axis.title = element_text(size = rel(1)),
          legend.text = element_text(size = rel(1)),
          legend.position = "top") +
    guides(fill = guide_legend(nrow = 1))
ggsave(file.path(fg_path, "param_tuning.png"), 
       width = 6, height = 4, dpi = 500)
```

According to the figure above, the optimal sample sizes are: 0.4 at 10 arc-minutes, 0.2 at 5 arc-minutes, and 0.3 at 2.5 arc-minutes.

#### Simulation

```{r}
# Define a function to thin the occurrences
occ_thin <- function(occ, cells_thin_to, seed = 123) {
    # Thin occurrence
    occ_ints <- st_intersects(cells_thin_to, occ)
    do.call(rbind, lapply(1:length(occ_ints), function(n) {
        each <- occ_ints[[n]]
        if (length(each) != 0) {
            if (length(each) == 1) {
                occ %>% slice(each)
            } else {
                set.seed(seed + n)
                ids <- sample(each, size = 1)
                occ %>% slice(ids)
            }
        }
    }))
}

# Read variables
occ <- read_sf(file.path(data_path, "observations/ele_occurrence.geojson"))
scale <- 10
vars <- stack(
    file.path(hr_path, sprintf("variables_final_%s.tif", scale)))

# Make a non-NA mask
msk <- lapply(1:nlayers(vars), function(n) !is.na(vars[[n]]))
msk <- sum(stack(msk)) == nlayers(vars)
msk[msk == 0] <- NA
cells_thin_to <- vars[[1]]
values(cells_thin_to) <- 1:ncell(cells_thin_to)
cells_thin_to <- cells_thin_to * msk
cells_thin_to <- rasterToPolygons(cells_thin_to) %>% st_as_sf()
rm(msk)

# Refine the occurrence from fine to coarse scale
occ_thin_10fds <- lapply(1:10, function(n){
    occ_thin(occ, cells_thin_to, seed = 123 * n)
})

walk(c(10, 5, 2.5), function(scale) {
    ratio_sample <- switch(as.character(scale), 
                           '10' = 0.4, '5' = 0.2, '2.5' = 0.3)
    fname <- file.path(result_path, sprintf("tuning_cv_%s_%s.rda", 
                                        scale, ratio_sample))
    message(fname); load(fname)
    # Get the best parameters for different assessment metrics
    # Pick auc, auc_ratio, CBI, TSS and f-measure as the assessment
    # to pick the best params
    best_params <- tuning_cv %>% 
        arrange(desc(TSS), desc(auc_ratio), desc(auc), desc(`f-measure`)) %>%
        slice(1) %>% 
        select(sample_size, max_depth, ndim, scoring_metric)
    
    # run models
    modeling_cs(best_params, census_block, occ_thin_10fds, 
                scale, ratio_sample, hr_path, result_path,
                seed = 123 + scale * 10)
})
```

#### Gather results

So, expected results from this part are the important variables, how elephants correspond to important variables, and the best scale in 2.5, 5, and 10 minutes.

Then remove the not important and not responsive variables and use the best scale to make the final map at coarse scale. This result will be used as prior probability in the next part.

##### Model evaluation

Both train and test evaluation are important and should be considered in this step. 5-minute scale works slightly better in many metrics (AUC, f-measure), particularly over train dataset. It means the model can describe well the pseudo dataset. So we decide to use 5-minute scale as the optimal scale to be the prior probability.

```{r}
library(tidyverse)
fg_path <- here("docs/figures")

tabularize_evaluation <- function(eval, type = "train") {
    if (type == "train") {
        tibble(
            CBI = eval$po_evaluation$boyce$cor,
            `AUC (ratio)` = eval$po_evaluation$roc_ratio$auc_ratio,
            TSS = eval$pb_evaluation$TSS$`Optimal TSS`,
            AUC = eval$pb_evaluation$roc$auc,
            Sensitivity = eval$pb_evaluation$sensitivity,
            `f-measure` = eval$pb_evaluation$`f-measure`)
    } else {
        tibble(
        Specificity = eval$pb_evaluation$specificity)
    }
}

evals_train <- do.call(rbind, lapply(c(10, 5, 2.5), function(scale){
    load(file.path(result_path, sprintf("runs_%s.rda", scale)))
    
    do.call(rbind, lapply(runs, function(run) {
        tabularize_evaluation(run$eval_test)
    })) %>% mutate(scale = scale)
}))

evals_sensitivity <- do.call(rbind, lapply(c(10, 5, 2.5), function(scale){
    load(file.path(result_path, sprintf("stt_real_same_%s.rda", scale)))
    
     do.call(rbind, lapply(sensitivity_real, function(each) {
        data.frame("Sensitivity" = each)
    })) %>% mutate(scale = scale)
}))

# Check and compare the results
evals_train <- evals_train %>% 
    pivot_longer(cols = c(`AUC (ratio)`, AUC, `f-measure`),
                 names_to = "metrics", values_to = "value") %>% 
    mutate(scale = factor(scale, levels = c(2.5, 5, 10)))

evals_sensitivity <- evals_sensitivity %>% 
    pivot_longer(cols = c(Sensitivity),
                 names_to = "metrics", values_to = "value") %>% 
    mutate(scale = factor(scale, levels = c(2.5, 5, 10)))

g_train <- ggplot(evals_train, 
       aes(x = metrics, y = value, fill = scale)) +
    geom_boxplot(outlier.colour = "white", outlier.shape = 8,
                 outlier.size = 2) +
    scale_fill_brewer("Scale", palette = "Dark2") +
    ylab('') +
    xlab('') +
    theme_classic() +
    theme(text = element_text(size = 14),
          axis.text.x = element_text(angle = 0),
          axis.text = element_text(size = rel(0.8)),
          axis.title = element_text(size = rel(1)),
          legend.text = element_text(size = rel(1)),
          legend.position = "top") +
    guides(fill = guide_legend(nrow = 1))

g_eval_stt <- ggplot(evals_sensitivity, 
                  aes(x = metrics, y = value, fill = scale)) +
    geom_boxplot(outlier.colour = "white", outlier.shape = 8,
                 outlier.size = 2) +
    scale_fill_brewer("Scale", palette = "Dark2") +
    ylab('') +
    xlab('') +
    theme_classic() +
    theme(text = element_text(size = 14),
          axis.text.x = element_text(angle = 0),
          axis.text = element_text(size = rel(0.8)),
          axis.title = element_text(size = rel(1)),
          legend.text = element_text(size = rel(1)),
          legend.position = "top") +
    guides(fill = guide_legend(nrow = 1))

library(ggpubr)
figure <- ggarrange(g_train, g_eval_stt, 
                    labels = c("A", "B"),
                    widths = c(2, 1),
                    common.legend = TRUE)

# Annotate the figure by adding a common labels
annotate_figure(figure,
                bottom = text_grob("Evaluation metrics", 
                                   color = "black", 
                                   size = 14, vjust = -1))

ggsave(file.path(fg_path, "eval_train_test.png"), 
       width = 6, height = 4, dpi = 500, bg = "white")
```

##### Variable importance based on Shapley values

```{r}
library(tidytext)

# Fix a name lost bug here
nms <- c(paste0("BIO", c(1, 4, 7, 12, 15)), 
         "NDVI (dry season)", "NDVI (wet season)", "NDVI seasonality",
         "Road density", "River density", "Settlement density",
         "Surface roughness", "Patch density", "Patch richness density",
         "Shannon's diversity index", "Cropland edge density", 
         "Edge density of closed canopy habitat", 
         "Mean of Contiguity index (open habitat)", 
         "Patch density (open habitat)", "Coverage of open habitat", 
         "Coverage of waterbodies")

abs_mean <- function(v) mean(abs(v))
var_imp <- do.call(rbind, lapply(c(10, 5, 2.5), function(scale){
    load(file.path(result_path, sprintf("runs_%s.rda", scale)))
    
    do.call(rbind, lapply(runs, function(run) {
        shap_train <- run$variable_analysis$SHAP$train %>% 
            summarise(across(all_of(names(.)), abs_mean))
        names(shap_train) <- nms
        shap_test <- run$variable_analysis$SHAP$test %>% 
            summarise(across(all_of(names(.)), abs_mean))
        names(shap_test) <- nms
        rbind(as_tibble(shap_train) %>% mutate(type = "train"),
        as_tibble(shap_test) %>% mutate(type = "test"))
    })) %>% mutate(scale = scale)
}))

## Reshape the table
var_imp <- var_imp %>% 
    pivot_longer(cols = rev(names(var_imp))[-c(1, 2)],
                 names_to = "variable", values_to = "value") %>% 
    mutate(scale = factor(scale, levels = c(2.5, 5, 10))) %>% 
    mutate(type = factor(type, levels = c("train", "test")))
var_imp <- var_imp %>% group_by(type, scale, variable) %>% 
    mutate(value = mean(value)) %>% ungroup()
var_imp_overall <- var_imp %>% group_by(variable) %>% 
    summarise(value = mean(value)) %>% 
    mutate(scale = "overall")
var_imp <- var_imp %>% group_by(scale, variable) %>% 
    summarise(value = mean(value)) %>% 
    rbind(var_imp_overall) %>% 
    mutate(value = value * 1000) %>% 
    mutate(scale = factor(scale, levels = c("overall", "10", "5", "2.5")))

# Check the overall contribution among all three scales
# and check each scale
facet_names <- c("Overall", "10 minutes", "5 minutes", "2.5 minutes")
names(facet_names) <- c("overall", "10", "5", "2.5")

ggplot(var_imp, 
       aes(x = reorder_within(variable, value, scale), 
           y = value)) +
    geom_bar(stat = "identity", color = "transparent", 
             fill = "black", position = position_dodge()) +
    ylab('mean(|Shapley value|) * 1000') +
    xlab('Environmental variable') +
    scale_x_reordered() +
    coord_flip() +
    facet_wrap(~scale, labeller = labeller(scale = facet_names), 
               scales = "free_y") +
    theme_classic() +
    theme(axis.text.x = element_text(angle = 0),
          axis.text = element_text(size = rel(0.8), color = "black"),
          axis.title = element_text(size = rel(1)),
          legend.text = element_text(size = rel(1)),
          legend.position = "top") +
    guides(fill = guide_legend(nrow = 1))

ggsave(file.path(fg_path, "var_imp.png"), 
       width = 8, height = 6, dpi = 500)
```

##### Variable corresponses

```{r}
var_rsp <- do.call(rbind, lapply(c(10, 5, 2.5), function(scale){
    load(file.path(result_path, sprintf("runs_%s.rda", scale)))
    
    do.call(rbind, lapply(runs, function(run) {
        shaps <- run$shap_dependences$dependences_cont
        names(shaps) <- nms
        do.call(rbind, lapply(nms, function(nm) {
            shaps[[nm]] %>% mutate(variable = nm)
        }))
    })) %>% mutate(scale = scale)
}))

## Reformat the table
var_rsp <- var_rsp %>% 
    mutate(scale = factor(scale, levels = c("2.5", "5", "10")))

## Plot (a template)
ggplot(var_rsp %>% filter(variable == "BIO1")) +
    geom_point(aes(x = x, y = y, color = scale), size = 0.5) +
    # geom_smooth(aes(x = x, y = y, color = scale),
    #             span = 0.3, alpha = 0) +
    scale_color_brewer("Scale", palette = "Dark2") +
    ylab('Shapley value') +
    xlab('Environmental variable value') +
    # facet_wrap(~variable, scales = "free") +
    theme_classic() +
    theme(axis.text.x = element_text(angle = 0),
          axis.text = element_text(size = rel(0.8)),
          axis.title = element_text(size = rel(1)),
          legend.text = element_text(size = rel(1)),
          legend.position = "top") +
    guides(fill = guide_legend(nrow = 1))
```

BIO1 (annual mean temperature): the optimal values are from 18 to 25 degree. The elephants correspond to it very similarly in three scales for large values. But for values below optimal, it affects elephant more mild at large scale.

BIO4 (temperature seasonality): the optimal values are 1.2 - 1.8 degree. Elephants correspond to it almost the same at three scales.

BIO7 (temperature annual range): the optimal values are 15 - 18 degree. Elephants correspond to it almost the same at three scales.

BIO12 (annual precipitation): the optimal values are 600 - 1200. Elephants correspond to it almost the same at three scales.

BIO15 (precipitation seasonality): the optimal values are 80 - 110. Elephants correspond to it almost the same at three scales. Except at fine scale (2.5), it has severe impact on elephants when the values are lower than optimal.

NDVI (dry season): optimal values are 0.25 - 0.6. It works similarly at three scales. The values above 0.5 may indicate tree existence.

NDVI (wet season): optimal values are 0.5 - 0.75. It works similarly at three scales.

NDVI seasonality: optimal values are 0.1 - 0.2. It works similarly at three scales. But the curve has an obvious shift from left to right when the scale changes from 2.5 to 10. It may indicate that at finer scale, elephants have less tolerance of seasonality of food resource.

River density: it works similarly at three scales. 

Road density: it works similarly at three scales. 

Settlement density: it works similarly at three scales. 

Cropland edge density: works similar with road density and settlement density. Road, settlement, and cropland edge density corresponds indicate that elephant is a species that coexist with humans.

Surface roughness: it has obvious scale dependence. It has smoother impact at larger scale.

Patch density: it works similarly at three scales. Optimal values are around 100.

Patch richness density: this one has the most obvious scale dependence. It makes sense because this metrics describes the overall richness within a landscape. So the large the landscape, the higher it should be.

Shannon's diversity index: works similarly at three scales. But the curve has an obvious shift from left to right when the scale changes from 2.5 to 10. 

Tree edge density: under 150, it has a stable correspond with elephants. When it is larger than 150, elephant suitability starts to drop. So tree is an important food source and shade for elephants.

Contiguity index distribution (Savanna): it has an obvious scale dependence. The optimal values are the same, which are around 0.5. The curve is smoother when the scale is smaller. This metrics assess the spatial connectedness of cells in patches. So it indicates when the scale is large, elephants prefer to stay in a diverse environment.

Savanna patch density: works similarly at three scales. And the suitability drops when savanna patch density decreases. Because savanna is elephants' favorite (has to?) habitat.

Ratio of Savanna: works opposite to savanna patch density.

Ratio of water: works similarly at three scales and it affect habitat suitability negatively.

##### Predicted environmental suitability at best coarse scales

```{r}
walk(c(10, 5, 2.5), function(scale) {
    load(file.path(result_path, sprintf("runs_%s.rda", scale)))
    prediction <- merge(do.call(c, lapply(runs, function(run) {
        run$prediction
    }))) %>% st_apply(c("x", "y"), mean, na.rm = TRUE)
    
    fname <- file.path(result_path, sprintf("landscape_utility_%s.tif", scale))
    write_stars(prediction, fname)
})
```

### Prediction on finer scale

According to last part, the optimal scale to fit an SDM with expert range map is 5 minute. The logic of this step is to use the model fitted with 5 minute on a finer scale with 5 minute buffer at each pixel to predict the suitability. So the heaviest work in this section is to prepare datasets for prediction. Here, we select 30s as the fine scale.

A recall of all variables:

- BIO1 = Annual Mean Temperature
- BIO4 = Temperature Seasonality (standard deviation ×100)
- BIO7 = Temperature Annual Range (BIO5-BIO6)
- BIO12 = Annual Precipitation
- BIO15 = Precipitation Seasonality (Coefficient of Variation)
- Mean NDVI during wet season
- Mean NDVI during dry season
- NDVI Seasonality (standard deviation)
- Settlement density
- Roads density
- River density
- SD of elevation
- Patch density (PD)
- Patch richness density (PRD)
- Shannon’s diversity index (SHDI)
- Ratio of Savanna (Shrub, grassland, and wetland)
- Ratio of Water
- Edge density (ED) of cropland
- Edge density (ED) of tree
- Contiguity index distribution (mean) (CONTIG_MN) of savanna
- Patch density (PD) of savanna

#### Fine scale NDVI

```{r}
library(googledrive)

# 30m
fnames_gdr <- drive_find(
    q = "name contains 'lansat8_sr_13_19_NDVI_mean_sd_30m' or 
    name contains 'lansat8_sr_13_19_wet_NDVI_mean_30m' or 
    name contains 'lansat8_sr_13_19_dry_NDVI_mean_30m'")

walk(1:nrow(fnames_gdr), function(n) {
    fname <- fnames_gdr$name[n]
    id_to_dl <- fnames_gdr$id[n]
    message(fname)
    drive_download(as_id(id_to_dl),
                   path = file.path(data_path, "NDVI", fname),
                   overwrite = TRUE)
}); rm(fnames_gdr)

fnames <- list.files(file.path(data_path, "NDVI"),
                     pattern = "NDVI_mean_sd_30m", full.names = TRUE)
ndvi_sd <- do.call(merge, lapply(fnames, rast))

fnames <- list.files(file.path(data_path, "NDVI"),
                     pattern = "wet_NDVI_mean_30m", full.names = TRUE)
ndvi_wet <- do.call(merge, lapply(fnames, rast))

fnames <- list.files(file.path(data_path, "NDVI"),
                     pattern = "dry_NDVI_mean_30m", full.names = TRUE)
ndvi_dry <- do.call(merge, lapply(fnames, rast))
ndvis <- c(ndvi_dry, ndvi_wet, ndvi_sd)
names(ndvis) <- c("ndvi_dry_season", "ndvi_wet_season", "ndvi_seasonality")

ndvis <- ndvis * 100000
writeRaster(ndvis, file.path(data_path, "NDVI/lansat8_sr_13_19_NDVI_30m.tif"),
            datatype = "INT4S", overwrite = TRUE, 
            wopt = list(gdal=c("COMPRESS=LZW")))
```

#### Prepare all variables at fine scale

Split the huge vectors for speed

```{r}
template <- rast(file.path(data_path, "worldclim/bios_0.5.tif"), lyrs = 1)
template <- aggregate(template, fact = 50)
values(template) <- 1:ncell(template)
template <- raster(template)

# Read layers
buildings <- read_sf(file.path(vct_path, "buildings.geojson"))
buildings <- vect(buildings)
buildings <- centroids(buildings, inside = FALSE)
buildings <- wrap(buildings)
roads <- vect(file.path(vct_path, "roads.geojson"))
roads <- roads[, -c(1:ncol(roads))]
roads <- wrap(roads)
rivers <- rbind(vect(file.path(vct_path, "rivers.geojson")),
                vect(file.path(vct_path, "streams.geojson")))
rivers <- rivers[, -c(1:ncol(rivers))]
rivers <- wrap(rivers)

n_col <- ncol(template); crs_to <- crs(template)
xre <- res(template)[1]; yre <- res(template)[2]
xre_buf <- 0.08333333; yre_buf <- 0.08333333
xmin <- extent(template)[1]; ymax <- extent(template)[4]
pbmclapply(1:ncell(template), function(id) {
    x_id <- id %% n_col; y_id <- id %/% n_col
    x_min <- xmin + xre * (x_id - 1) - xre_buf
    x_max <- x_min + xre + 2 * xre_buf
    y_max <- ymax - yre * y_id + yre_buf
    y_min <- y_max - yre - 2 * yre_buf
    
    blds <- crop(vect(buildings), ext(x_min, x_max, y_min, y_max))
    # rds <- crop(vect(roads), ext(x_min, x_max, y_min, y_max))
    # rvs <- crop(vect(rivers), ext(x_min, x_max, y_min, y_max))
    
    writeVector(blds, sprintf("data/temp/blds_%s.geojson", id),
                filetype = "GeoJSON", options = NULL)
    # writeVector(rds, sprintf("data/temp/rds_%s.geojson", id),
    #             filetype = "GeoJSON", options = NULL)
    # writeVector(rvs, sprintf("data/temp/rvs_%s.geojson", id),
    #             filetype = "GeoJSON", options = NULL)
}, mc.cores = 2)

# Make a catalog
library(stringr)
names(template) <- "index"
catalog <- rasterToPolygons(template) %>% st_as_sf()

blds_nm <- list.files("data/temp", pattern = "blds", full.names = TRUE) %>% 
    data.frame(bld_path = .) %>% 
    mutate(index = as.integer(str_extract(bld_path, "[0-9]+")))
rds_nm <- list.files("data/temp", pattern = "rds", full.names = TRUE) %>% 
    data.frame(rd_path = .) %>% 
    mutate(index = as.integer(str_extract(rd_path, "[0-9]+")))
rvs_nm <- list.files("data/temp", pattern = "rvs", full.names = TRUE) %>% 
    data.frame(rv_path = .) %>% 
    mutate(index = as.integer(str_extract(rv_path, "[0-9]+")))
catalog <- left_join(catalog, blds_nm, by = "index") %>% 
    left_join(., rds_nm, by = "index") %>% 
    left_join(., rvs_nm, by = "index")

write_sf(catalog, "data/temp/catalog.geojson")
```

```{r}
library(landscapemetrics)
library(parallel)
library(tidyverse)

temp <- rast(file.path(data_path, "worldclim/bios_0.5.tif"), lyrs = 1)
values(temp) <- 1:ncell(temp)

# Elevation
dsm <- raster(file.path(rst_path, "dsm_tanzania.tif"))

# landscape
lc <- raster(file.path(data_path, "landcover/landcover.tif"))
lc_class <- raster(
    file.path(data_path, "landcover/crop_tree_savanna_water.tif"))

template <- raster(temp)
n_col <- ncol(template); crs_to <- crs(template)
xre <- res(template)[1]; yre <- res(template)[2]
xre_buf <- xre * 10; yre_buf <- yre * 10
xmin <- extent(template)[1]; ymax <- extent(template)[4]

catalog <- read_sf("data/temp/catalog.geojson")

dst_path <- file.path(data_path, "vars_patch", "variables_buf.tif")
csv_path <- gsub(".tif", ".csv", dst_path)
file.create(csv_path)
write.table(setNames(data.frame(matrix(ncol = 22, nrow = 0)), 
                     c("id", "bio1", "bio4", "bio7", "bio12", "bio15",
                       "ndvi_dry_season", "ndvi_wet_season", "ndvi_seasonality",
                       "settlement_density", "roads_density", "rivers_density",
                       "sd_elevation", "pd", "prd", "shdi", 
                       "cropland_ed", "tree_ed", "savanna_contig_mn",
                       "savanna_pd", "savanna_ratio", "water_ratio")), 
            file = csv_path, append = TRUE, 
            sep = ',', row.names = FALSE)

sf::sf_use_s2(FALSE)
vals <- do.call(rbind, pbmclapply(1:ncell(template), function(id) {
    x_id <- id %% n_col; y_id <- id %/% n_col
    
    # Pixel
    x_min <- xmin + xre * (x_id - 1); x_max <- x_min + xre
    y_max <- ymax - yre * y_id; y_min <- y_max - yre
    
    # Get pixel bbox
    buf <- st_bbox(c(xmin = x_min + 0.00001, xmax = x_max - 0.00001, 
                     ymax = y_max - 0.00001, ymin = y_min + 0.00001), 
                   crs = crs_to) %>% st_as_sfc()
    ids <- st_intersects(buf, catalog) %>% unlist() %>% unique()
    
    # centroid
    x_ctd <- x_min + (x_max - x_min) / 2
    y_ctd <- y_min + (y_max - y_min) / 2
    
    # Buffered area
    x_min <- x_ctd - xre_buf / 2; x_max <- x_ctd + xre_buf / 2
    y_min <- y_ctd - yre_buf / 2; y_max <- y_ctd + yre_buf / 2
    
    # Get the buffered area
    blk <- rast(raster(matrix(1),
                       xmn = x_min, xmx = x_max,
                       ymn = y_min, ymx = y_max, 
                       crs = crs_to))
    
    # Start to calculate the variables
    ## BIOs
    bios <- rast(file.path(data_path, "worldclim/bios_0.5.tif"))
    bios <- subset(bios, c(1, 4, 7, 12, 15))
    bios <- crop(bios, blk)
    bios <- as.data.frame.list(
        values(bios) %>% colMeans(na.rm = TRUE)) %>% as_tibble()
    
    # NDVIs
    ndvis <- rast(file.path(data_path, "NDVI/lansat8_sr_13_19_NDVI_30m.tif"))
    if (relate(as.polygons(ext(ndvis)), 
                       as.polygons(ext(blk)), "intersects")) {
        ndvis <- crop(ndvis, blk)
        ndvis <- as.data.frame.list(
            values(ndvis) %>% colMeans(na.rm = TRUE)) %>% as_tibble()
    } else ndvis <- tibble(ndvi_dry_season = NA,
                           ndvi_wet_season = NA,
                           ndvi_seasonality = NA)
    
    if (length(ids) != 0) {
        # Densities
        catalog_sub <- catalog %>% slice(ids)
        
        if (is.na(catalog_sub$bld_path)) {
            blds <- tibble(settlement_density = 0)
        } else {
            blds <- crop(vect(catalog_sub$bld_path), 
                         ext(x_min, x_max, y_min, y_max))
            blds <- tibble(settlement_density = nrow(blds))
        }
        
        if (is.na(catalog_sub$rd_path)) {
            rds <- tibble(roads_density = 0)
        } else {
            rds <- crop(vect(catalog_sub$rd_path), ext(x_min, x_max, y_min, y_max))
            rds <- tibble(roads_density = sum(perim(rds)) / 1000)
        }
        
        if (is.na(catalog_sub$rv_path)) {
            rvs <- tibble(rivers_density = 0)
        } else {
            rvs <- crop(vect(catalog_sub$rv_path), ext(x_min, x_max, y_min, y_max))
            rvs <- tibble(rivers_density = sum(perim(rvs)) / 1000)
        }
    } else {
        blds <- tibble(settlement_density = 0)
        rds <- tibble(roads_density = 0)
        rvs <- tibble(rivers_density = 0)
    }
    
    # SD of elevation
    if (relate(as.polygons(ext(dsm)), 
                       as.polygons(ext(blk)), "intersects")) {
        dsm_blk <- crop(rast(dsm), blk)
        dsm_blk <- tibble(sd_elevation = sd(values(dsm_blk), na.rm = TRUE))
    } else dsm_blk <- tibble(sd_elevation = NA)
    
    # Landscape
    metrics_l <- c("lsm_l_pd", "lsm_l_prd", "lsm_l_shdi")
    metrics_c <- c("lsm_c_contig_mn", "lsm_c_pd", "lsm_c_ed")
    
    blk_lc <- project(blk, crs(rast(lc)))
    # Do calculation for each window size
    if (relate(as.polygons(ext(lc)), 
               as.polygons(ext(blk_lc)), "intersects")) {
        # Crop the landscape
        lc_blk <- crop(rast(lc), blk_lc)
        
        # Calculate the metrics
        if (isTRUE(global(lc_blk, fun = "isNA") != ncell(lc_blk))) {
            lc_metrics_l <- calculate_lsm(lc_blk, what = metrics_l) %>% 
                dplyr::select(metric, value) %>% 
                pivot_wider(names_from = metric)
        } else {
            lc_metrics_l <- tibble(pd = NA, prd = NA, shdi = NA)
        }
    } else lc_metrics_l <- tibble(pd = NA, prd = NA, shdi = NA)
    
    # Read ocean mask
    ocean <- vect("data/vectors/ocean_mask.geojson")
    ocean <- project(ocean, crs(blk_lc))
    
    # Do calculation for each window size
    if (relate(as.polygons(ext(lc_class)), 
               as.polygons(ext(blk_lc)), "intersects")) {
        # Crop the landscape
        lc_blk <- crop(rast(lc_class), blk_lc)
        
        if (relate(as.polygons(ext(blk_lc)), 
                   ocean, "intersects")) {
            lc_blk <- terra::mask(lc_blk, ocean, inverse = TRUE)
        }
        
        # Calculate the metrics
        if (isTRUE(global(lc_blk, fun = "isNA") != ncell(lc_blk))) {
            values <- calculate_lsm(lc_blk, what = metrics_c)
            
            # Add NA for missing class
            ## some hardcoded values here
            classes <- unique(values$class)
            missing_class <- setdiff(1:3, classes)
            if (length(missing_class) > 0) {
                nms <- unique(values$metric)
                values <- rbind(
                    values,
                    data.frame(
                        layer = 1,
                        level = "class",
                        class = sort(rep(missing_class, length(nms))),
                        id = NA,
                        metric = rep(nms, length(missing_class)),
                        value = NA)) %>% 
                    arrange(metric, class)}
            
            # Group metrics
            crop_tree <- values %>% 
                filter(class %in% c(1, 2) & 
                           metric == "ed") %>%
                mutate(metric = paste(class, metric, sep = "_")) %>%
                dplyr::select(metric, value) %>% 
                pivot_wider(names_from = metric)
            savanna <- values %>% 
                filter(class %in% c(3) & 
                           metric %in% c("pd", "contig_mn")) %>%
                mutate(metric = paste(class, metric, sep = "_")) %>%
                dplyr::select(metric, value) %>% 
                pivot_wider(names_from = metric)
            
            # ratio
            ratios <- data.frame(freq(lc_blk)[, c("value", 'count')])
            ratios <- right_join(
                ratios, data.frame(value = 3:4, 
                                   metric = paste(3:4, "ratio", 
                                                  sep = "_")),
                by = "value") %>% arrange(value) %>% 
                mutate(value = ifelse(is.na(count), 0, 
                                      count / ncell(lc_blk))) %>% 
                dplyr::select(value, metric) %>% 
                pivot_wider(names_from = metric)
            
            lc_metrics_c <- as_tibble(cbind(crop_tree, savanna, ratios))
        } else {
            lc_metrics_c <- tibble("1_ed" = 0, "2_ed" = 0, "3_contig_mn" = 0,
                                   "3_pd" = 0, "3_ratio" = 0, "4_ratio" = 0)
        }
    } else lc_metrics_c <- tibble("1_ed" = 0, "2_ed" = 0, "3_contig_mn" = 0,
                                  "3_pd" = 0, "3_ratio" = 0, "4_ratio" = 0)
    
    nms <- gsub("1", "cropland", names(lc_metrics_c))
    nms <- gsub("2", "tree", nms)
    nms <- gsub("3", "savanna", nms)
    nms <- gsub("4", "water", nms)
    names(lc_metrics_c) <- nms
    
    # Collect all of them
    rst <- cbind(bios, ndvis, blds, rds, rvs, 
                 dsm_blk, lc_metrics_l, lc_metrics_c)
    write.table(cbind(data.frame(id = id), rst), 
                file = csv_path, append = TRUE, 
                sep = ',', row.names = FALSE, 
                col.names = FALSE)
    rst
}, mc.cores = 12))

# Burn values into raster stack
message("Burn the values into raster stack.")
save(vals, file = gsub(".tif", ".rda", dst_path))
vars <- rep(temp, ncol(vals))
values(vars) <- as.matrix(vals)
names(vars) <- names(vals)
# Modify the order of variables
vars <- subset(vars, c(1:8, 10, 11, 9, 12:21))
writeRaster(vars, dst_path)
```

#### Prediction

```{r}
library(stars)
library(itsdm)

# Load model
load(file.path(result_path, "runs_5.rda"))

# Load variables
variables <- read_stars(
    file.path(data_path, "vars_patch", "variables_buf.tif")) %>% 
    split("band")
    
# Do prediction
predictions <- merge(do.call(c, lapply(runs, function(run) {
        probability(run$model, variables)})))
prediction <- st_apply(predictions, c("x", "y"), mean, na.rm = TRUE)
prediction_sd <- st_apply(predictions, c("x", "y"), sd, na.rm = TRUE)
prediction <- c(prediction, prediction_sd)
names(prediction) <- c("prediction", "standard deviation")
    
fname <- file.path(result_path, "landscape_utility_10km_to_1km.tif")
write_stars(merge(prediction), fname)
```
