---
title: "Environmental space at hierarchical scales"
author: "Lei Song"
date: "6/1/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(sf)
library(stars)
library(ggplot2)

data_path <- here("data")
result_path <- here("results")
```

## Introduction

We will first target the climatic domain (or environmental space) of African elephant at different scales using Isolation Forest. Variables at broad scale (e.g. climatic vars) and variables with long history (e.g. NDVI and its trend) will be used in this step. The idea is to first target the highly possible suitable environmental areas (like fundamental niche), then to detail the environmental suitability according to other environmental constraints (like realistic niche).

## 1000m scale

```{r}
library(sf)
library(stars)
library(dplyr)
library(tmap)
library(stringr)
library(itsdm, quietly = TRUE)
library(caret)
library(parallel)

# Read variables for environmental space modeling
## Resources (NDVI and water), climate, and topography
vars <- read_stars(file.path(data_path, "variables_1000m.tif"))
vars <- split(vars, "band")
nms <- names(vars)
nms <- nms[str_detect(nms, "ndvi|bio|DSM|slope|roughness|TRI")]
vars <- vars %>% select(all_of(nms)); rm(nms)
template <- vars[1]; nms <- names(template)
template <- template %>% mutate({{nms}} := NA)
n_pseudo_block <- read_sf(
        file.path(data_path, "population/n_pseudo_block.geojson"))

# Check the variables
tm_shape(vars) + tm_raster()

# Parameter tuning
## Important parameters: 
## prob_pick_pooled_gain (prefer to lower value here) - 0, 0.1, 0.2
## ntry (only if use prob_pick_pooled_gain) - 1, 10, 15, 20
## sample_size (prefer to higher value here) - 0.8, 0.9, 1.0
## max_depth (prefer to slightly higher value) - 15, 20, 30
## ndim (recommended to use small value) - 2, 3, 4
## scoring_metric: depth, adj_depth
## Since the objective is to rank the environmental conditions, not to
## really isolate the outliers, so not use prob_pick_pooled_gain and ntry.
## max_depth is rather be higher. 
## Not use density based methods because the density of elephants in Tanzania
## is not linear related to the environmental conditions.

# Define the sets of preferred parameters
params <- expand.grid(
    sample_size = c(0.8, 0.9, 1.0),
    max_depth = c(15, 20, 25, 30),
    ndim = c(2, 3, 4),
    scoring_metric = c("depth", "adj_depth"))

tuning_cv <- do.call(rbind, lapply(1:nrow(params), function(n) {
    message(sprintf("Run %s out of %s", n, nrow(params)))
    
    # Parameters
    params_run <- params[n, ]
    
    # Dynamically make pseudo samples
    set.seed(n)
    pseudo_occ <- st_sample(
        n_pseudo_block, size = n_pseudo_block$sample) %>% 
        st_as_sf()
    pseudo_occ <- st_rasterize(pseudo_occ, template) %>%
        st_xy2sfc(as_points = T) %>% st_as_sf() %>%
        select(geometry)
    
    # Split occurrence with 5 folds
    set.seed(123)
    flds <- createFolds(1:nrow(pseudo_occ), 
                        k = 5, list = TRUE, 
                        returnTrain = FALSE)
    
    # Cross validation
    it_sdms <- mclapply(flds, function(ids) {
        # Split
        occ_sf <- pseudo_occ[setdiff(1:nrow(pseudo_occ), ids), ]
        occ_test_sf <- pseudo_occ[ids, ]
        
        # Do modeling
        isotree_po(
            occ = occ_sf,
            occ_test = occ_test_sf,
            variables = vars,
            sample_size = params_run$sample_size,
            ndim = params_run$ndim, 
            max_depth = params_run$max_depth,
            scoring_metric = params_run$scoring_metric,
            seed = 10L,
            response = FALSE,
            spatial_response = FALSE,
            check_variable = FALSE,
            visualize = FALSE)}, mc.cores = 5)
    
    # Collect results
    eval_mean <- do.call(rbind, lapply(it_sdms, function(run) {
        eval_test <- run$eval_test
        tibble("cvi25" = eval_test$po_evaluation$cvi$`cvi with 0.25`,
               "cvi50" = eval_test$po_evaluation$cvi$`cvi with 0.5`,
               "cvi75" = eval_test$po_evaluation$cvi$`cvi with 0.75`,
               "cbi" = eval_test$po_evaluation$boyce$cor,
               "auc_ratio" = eval_test$po_evaluation$roc_ratio$auc_ratio,
               "sensitivity" = eval_test$pb_evaluation$sensitivity,
               "specificity" = eval_test$pb_evaluation$specificity,
               "TSS" = eval_test$pb_evaluation$TSS$`Optimal TSS`,
               "auc" = eval_test$pb_evaluation$roc$auc,
               `Jaccard's similarity index` = eval_test$pb_evaluation$`Jaccard's similarity index`,
               "f-measure" = eval_test$pb_evaluation$`f-measure`,
               `Overprediction rate` = eval_test$pb_evaluation$`Overprediction rate`,
               `Underprediction rate` = eval_test$pb_evaluation$`Underprediction rate`)
    })) %>% summarise(across(everything(), mean))
    
    # Return
    print(sprintf("CBI: %s, TSS: %s, auc_ratio: %s, auc: %s", 
                  eval_mean$cbi, eval_mean$TSS, 
                  eval_mean$auc_ratio, eval_mean$auc))
    cbind(params_run, eval_mean)
}))

save(tuning_cv, file = file.path(result_path, "tuning_cv.rda"))

# Monte Carlo simulation
# Pick auc, auc_ratio, CBI, TSS and f-measure as the assessment respectively
# to pick the best params
best_params <- rbind(tuning_cv %>% filter(auc == max(auc)),
                     tuning_cv %>% filter(auc_ratio == max(auc_ratio)),
                     tuning_cv %>% filter(cbi == max(cbi)),
                     tuning_cv %>% filter(TSS == max(TSS)),
                     tuning_cv %>% filter(`f-measure` == max(`f-measure`))) %>% 
    select(sample_size, max_depth, ndim, scoring_metric) %>% 
    unique()

# Start runs
num_cv <- 10
runs <- lapply(1:nrow(best_params), function(n){
    # Load parameters
    params <- best_params[n, ]
    
    # Print
    message(sprintf("Params: sample_size: %s, max_depth: %s, ndim: %s",
                    params$sample_size, params$max_depth, params$ndim))
    
    # Run 10 fold runs for each set of parameters
    cvs <- mclapply(1:num_cv, function(n_cv) {
        # Dynamically make pseudo samples
        set.seed(123 + n * n_cv)
        pseudo_occ <- st_sample(
            n_pseudo_block, size = n_pseudo_block$sample) %>% 
            st_as_sf()
        pseudo_occ <- st_rasterize(pseudo_occ, template) %>%
            st_xy2sfc(as_points = T) %>% st_as_sf() %>%
            select(geometry)
        
        # Split into training and test
        set.seed(234 + n * n_cv)
        ids <- sample(1:nrow(pseudo_occ), ceiling(nrow(pseudo_occ) * 0.2))
        occ_train <- pseudo_occ[setdiff(1:nrow(pseudo_occ), ids), ]
        occ_test <- pseudo_occ[ids, ]
        rm(pseudo_occ, ids)
        
        # Do modeling
        isotree_po(occ = occ_train,
                   occ_test = occ_test,
                   variables = vars,
                   ndim = params$ndim,
                   sample_size = params$sample_size,
                   max_depth = params$max_depth,
                   scoring_metric = params$scoring_metric,
                   seed = 10L,
                   # switch spatial response off
                   spatial_response = FALSE)
        }, mc.cores = 5)
    names(cvs) <- paste(
        sprintf("%s_%s_%s_%s", 
                params$sample_size, params$max_depth,
                params$ndim, params$scoring_metric), 
        1:num_cv, sep = "_")
    # Print one for checking
    print(cvs[[1]])
    
    # Return
    cvs
})

save(runs, file = file.path(result_path, "runs.rda"))
```
