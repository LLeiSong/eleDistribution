---
title: 'Part1: Landscape priority modeling at home range level'
author: "Lei Song"
date: '2022-07-12'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

## Introduction

Due to the limitation of available occurrence datasets, and also make the most of the existing valuable datasets (e.g. elephant census), we split the whole modeling process into a nested multi-scale structure, which mainly include two parts. Within part1, African elephants' preference of landscape at broad scale will be modeled. Expert range map and the elephant census dataset are used together to generate pseudo-occurrences for modeling. Because this part will do modeling in large scale, the distance-based features cannot perform well. So we use density-based features. These feature include roads, settlement, and rivers. Environmental variables include:

**Climate:**

- Selected bioclimatic variables (check)

**Vegetation:**

- Mean NDVI during wet season (check)
- Mean NDVI during dry season (check)
- NDVI Seasonality (standard deviation ×100) (check)

**Landscape structure:**

- Selected landscape metrics at landscape level (check)
- Selected landscape metrics at class level (check)
- Ratio of selected land cover types (check)
- Rivers/streams density (check)

**Anthropological impacts:**

- Settlement density (check)
- Road density (check)
- Cropland density

**Topographic features:**

- Terrain roughness (check)
- Slope (check)

## Elephant datasets
### Range map

The datasets are inquired from [elephant database](https://africanelephantdatabase.org). People want to use these datasets should inquired directly from them.

```{r}
library(sf)
library(here)
library(raster)
library(terra)
library(ggplot2)
library(purrr)
library(pbmcapply)
library(dplyr)

data_path <- here("data")
rst_path <- file.path(data_path, "rasters")
vct_path <- file.path(data_path, "vectors")
cs_path <- file.path(data_path, "vars_coarse_scale")
walk(c(rst_path, vct_path, cs_path), function(pth) {
    if (!dir.exists(pth)) dir.create(pth)
})

# Get the range map
range_map <- read_sf(
    file.path(data_path, "observations/elephant_database",
              "Range_Layer_bush/AED_Range_Layer_2015.shp")) %>% 
    filter(country == "Tanzania") %>% 
    select(range, area_sqkm)

# Check it
ggplot() + 
    geom_sf(data = range_map, aes(fill = range)) +
    scale_fill_brewer("Extant", palette = "Dark2") +
    theme_classic()

# Save out
write_sf(range_map, file.path(data_path, "observations/range_map.geojson"))
```

### Census

The census data is inquired from [elephant database](https://africanelephantdatabase.org) as well.

```{r}
# Read the data
census <- read_sf(
    file.path(data_path, 
              "observations/elephant_database/Input_Zones_Layer",
              "AED_Input_Zones_2016_Data_Sharing.shp")) %>% 
    filter(country == "Tanzania") %>% 
    # remove the too old ones
    filter(year > 2010) %>% 
    select(inpzone, stratum, estimate, area_sqkm)
census$estimate <- as.numeric(census$estimate)
census$area_sqkm <- as.numeric(census$area_sqkm)

# Calculate the population density
census <- census %>% mutate(density = estimate / area_sqkm)

# Check it
ggplot() + 
    geom_sf(data = census, aes(fill = density)) +
    scale_fill_viridis_c("Density") +
    theme_classic()

# Save out
write_sf(census, file.path(data_path, "observations/census.geojson"))
```

### Put them together

We will set weights to the pseudo occurrence samples generated by range and census dataset. Then how to set the weights? In census, we used population density as the weights. And use the average population density as the weight for range map where is not covered by census blocks. The area with population density 0 will be removed.

```{r}
# Assign the min population density
range_map <- st_difference(range_map, st_union(census)) %>% 
    st_union()
range_map <- range_map %>% st_as_sf() %>% 
    rename(geometry = x) %>% 
    mutate(area_sqkm = st_area(.) %>% units::set_units('km2'),
           density = min(census$density[census$density > 0])) %>% 
    mutate(estimate = as.integer(ceiling(density * area_sqkm)))

# Put together
pseudo_blocks <- rbind(range_map %>% select(estimate, density),
                       census %>% select(estimate, density))

# Check it
ggplot() + 
    geom_sf(data = pseudo_blocks, aes(fill = density)) +
    scale_fill_viridis_c("Density") +
    theme_classic()

# Save out
write_sf(pseudo_blocks, 
         file.path(data_path, "observations/pseudo_blocks.geojson"))
```

### Occurrence of African savanna elephant

We tried to find any public occurrence datasets to use. So far, GBIF occurrence dataset and a publicly available dataset related to genotype can be used as occurrence. In part1, the aggregated occurrence will be used as independent test dataset.

#### GBIF

Query the occurrence data using GBIF API. To reduce the impacts of landscape change, we only selected the occurrence after year 2015 to use. Then we cleaned the occurrence for any possible errors. 

```{r}
# remotes::install_github("ropensci/scrubr")
library(scrubr, quietly = TRUE)
library(lubridate, quietly = TRUE)
library(rgbif, quietly = TRUE)

## Set the time interval for querying on GBIF
start_year <- 2015
year <- sprintf('%s,%s',  start_year, year(Sys.Date()))

# Search
nm_search <- "Loxodonta africana (Blumenbach, 1797)"
occ <- occ_search(scientificName = nm_search,
                  country = "TZ",
                  hasCoordinate = TRUE,
                  limit = 200000,
                  year = year,
                  hasGeospatialIssue = FALSE)

# Clean the dataset
occ <- dframe(occ$data) %>%
    coord_impossible() %>%
    coord_incomplete() %>%
    coord_unlikely()

# Convert to sf
occ <- st_as_sf(occ[, c("decimalLongitude", "decimalLatitude")], 
                coords = c("decimalLongitude", "decimalLatitude"),
                crs = 4326)
rm(start_year, year, nm_search)
```

#### Genotype dataset

This dataset is shared by the paper *Genetic connectivity and population structure of African savanna elephants (Loxodonta africana) in Tanzania ([https://doi.org/10.1002/ece3.6728](https://doi.org/10.1002/ece3.6728))*. We converted the records to spatial points to use.

```{r}
fname <- file.path(data_path, "observations/genetype_dataset_15_17.csv")
occ2 <- read.csv(fname, stringsAsFactors = FALSE)
occ2 <- st_as_sf(occ2[, c("Lat", "Long")], 
                coords = c("Long", "Lat"),
                crs = 4326)
rm(fname)
```

#### Combine two datasets and clean

Combine two datasets and clip to the national boundary.

```{r}
occ <- rbind(occ, occ2); rm(occ2)
bry <- read_sf(file.path(data_path, "vectors/mainland_tanzania.geojson"))
occ <- occ[unique(unlist(st_intersects(bry, occ))), ]

# Save out and clean
write_sf(occ, file.path(data_path, "observations/ele_occurrence.geojson"))
rm(bry, occ)
```

## Environmental variables

Because of the limitation of range map/census map, we need to find out the most effective scale to use them. Thus, we tested scale 2.5 minutes, 5 minutes, 10 minutes and 20 minutes.

### Climate

Download Bioclimatic variables and check the correlation. According to literature, expert knowledge and data itself, we selected:

- BIO1 = Annual Mean Temperature
- BIO4 = Temperature Seasonality (standard deviation ×100)
- BIO7 = Temperature Annual Range (BIO5-BIO6)
- BIO12 = Annual Precipitation
- BIO15 = Precipitation Seasonality (Coefficient of Variation)

```{r}
library(itsdm)
library(stars)
library(purrr)
library(corrplot)

# Make a place to store data
cli_path <- file.path(data_path, "worldclim")
dir.create(cli_path)

# Download worldclim Bioclimatic variables.
sf_use_s2(FALSE)
bry <- read_sf(file.path(data_path, "vectors/mainland_tanzania.geojson"))
bry <- st_buffer(bry, 0.5)
walk(c(2.5, 5, 10), function(scale) {
    bios <- worldclim2(var = "bio", res = scale,
                   bry = bry, path = tempdir())
    write_stars(bios, 
                file.path(cli_path, sprintf("bios_%s.tif", scale)))
}); rm(bios)

# Analyze the correlation
fnames <- list.files(cli_path, full.names = TRUE)
walk(fnames, function(fname) {
    bios <- rast(fname)
    bios <- values(bios)
    bios <- data.frame(bios) %>% na.omit()
    corrplot(corr = cor(bios, method = "spearman"), 
             method = "square", type = "lower",
             diag = FALSE, addCoef.col = "black", 
             number.cex = 0.7, tl.cex = 0.7)
})
# The above figures show massive correlation between bio variables

# Subset our selected variables
walk(fnames, function(fname) {
    bios <- rast(fname)
    bios <- values(bios)
    bios <- data.frame(bios) %>% na.omit()
    bios <- bios %>% select(paste0("bio", c(1, 4, 7, 12, 15)))
    corrplot(corr = cor(bios, method = "spearman"), 
             method = "square", type = "lower",
             diag = FALSE, addCoef.col = "black", 
             number.cex = 0.7, tl.cex = 0.7)
})
# The figure above show healthy correlation between selected variables

# Save out
bry <- read_sf(file.path(data_path, "vectors/mainland_tanzania.geojson"))
bry <- vect(bry)
walk(fnames, function(fname) {
    bios <- rast(fname)
    bios <- subset(bios, c(1, 4, 7, 12, 15))
    bios <- mask(bios, bry, touches = TRUE)
    writeRaster(bios, file.path(cs_path, basename(fname)))
})
```

### Vegetation

- Mean NDVI during wet season
- Mean NDVI during dry season
- NDVI Seasonality (standard deviation)

We made the calculation in Google Earth Engine to take the advantage of it. Export from GEE, read the images, and resample to the dimension of bios.

```{r}
# Dry season
ndvi_dry <- rast(file.path(data_path, 
                           "NDVI/lansat8_sr_13_19_dry_NDVI_mean_1000m.tif"))
# In case there are any missing values
ndvi_dry <- focal(ndvi_dry, fun = mean, na.rm = TRUE, na.policy = "only")

# Wet season
ndvi_wet <- rast(file.path(data_path, 
                           "NDVI/lansat8_sr_13_19_wet_NDVI_mean_1000m.tif"))
ndvi_wet <- focal(ndvi_wet, fun = mean, na.rm = TRUE, na.policy = "only")

# Mean annual SD
ndvi_sd <- rast(file.path(data_path, 
                           "NDVI/lansat8_sr_13_19_NDVI_mean_sd_1000m.tif"))
ndvi_sd <- focal(ndvi_sd, fun = mean, na.rm = TRUE, na.policy = "only")
ndvis <- c(ndvi_dry, ndvi_wet, ndvi_sd)
names(ndvis) <- c("ndvi_dry_season", "ndvi_wet_season", "ndvi_seasonality")

writeRaster(ndvis, file.path(rst_path, "ndvi.tif"))

# Upscale
ndvis <- stack(ndvis)
walk(c(10, 5, 2.5), function(scale) {
    message(sprintf("Process scale: %s", scale))
    template <- raster(file.path(cs_path, sprintf("bios_%s.tif", scale)), 
                     band = 1)
    values(template) <- 1:ncell(template)
    vals <- do.call(rbind, pbmclapply(1:ncell(template), function(id) {
        blk <- classify(rast(template), cbind(id, 1), others = NA)
        blk <- trim(blk)
        
        # Do calculation for each window size
        if (relate(as.polygons(ext(ndvis)), 
                   as.polygons(ext(blk)), "intersects")) {
            # Crop the landscape
            ndvi_blk <- crop(rast(ndvis), blk)
            
            # Calculate the metrics
            if (isTRUE(global(ndvi_blk[[1]], fun = "isNA") != ncell(ndvi_blk))) {
                values(ndvi_blk) %>% colMeans() %>% rbind() %>% data.frame()
            } else {
                return(NA)
            }
        } else return(NA)
    }, mc.cores = detectCores() - 1))
    
    # Burn values to raster
    ndvi_out <- rep(rast(template), ncol(vals))
    values(ndvi_out) <- as.matrix(vals)
    names(ndvi_out) <- names(vals)
    dst_path <- file.path(cs_path, sprintf("ndvi_%s.tif", scale))
    writeRaster(ndvi_out, dst_path)
})
```

### Landscape structure
#### Landscape metrics

**Landscape level:**

- Aggregation index (AI)
- Patch density (PD)
- Marginal entropy (ENT)
- Conditional entropy
- Joint entropy
- Mutual information
- Edge density (ED)
- Patch richness density (PRD)
- Simpson’s diversity index (SIDI)
- Shannon’s diversity index (SHDI)

**Class level:**

Dense tree/cropland

- class ratio
- Edge density (ED)

Savanna (shrub + grass)

- Largest patch index (LPI)
- Patch density (PD)
- Contiguity index distribution (mean) (CONTIG_MN)

**Ratio of land cover types:**

- Cropland (one of anthropological impacts)
- Dense tree/forest
- Savanna (Shrub, grassland, and wetland)
- Water

```{r}
source(file.path(here("scripts"), "landscape_metrics.R"))

# Read datasets
lc <- rast(file.path(data_path, "landcover/landcover.tif"))
# Aggregate class types
# 1, cropland, 2, dense tree, 3, savanna (shrub + grass + wetland), 4, water
lc_class <- classify(lc, rbind(c(1, 1), c(2, 2),
                               c(3, 3), c(4, 3),
                               c(5, 4), c(8, 3)), others = NA)
writeRaster(lc_class, 
            file.path(data_path, "landcover/crop_tree_savanna_water.tif"),
            datatype = "INT1U")

# Landscape level
walk(c(10, 5, 2.5), function(scale) {
    message(sprintf("Process scale: %s", scale))
    template <- rast(file.path(cs_path, sprintf("bios_%s.tif", scale)), 
                  lyrs = 1)
    values(template) <- 1:ncell(template)
    dst_path <- file.path(cs_path, sprintf("lsp_l_metrics_%s.tif", scale))
    landscape_l_metrics(lc, template, dst_path)
}); rm(lc)

# Class level
lc_class <- rast(file.path(data_path, "landcover/crop_tree_savanna_water.tif"))
walk(c(10, 5, 2.5), function(scale) {
    message(sprintf("Process scale: %s", scale))
    template <- rast(file.path(cs_path, sprintf("bios_%s.tif", scale)), 
                     lyrs = 1)
    values(template) <- 1:ncell(template)
    dst_path <- file.path(cs_path, sprintf("lsp_c_metrics_%s.tif", scale))
    landscape_c_metrics(lc_class, template, dst_path)
}); rm(lc_class)
```

### Anthropological impacts and density features

- Settlement density
- Road density
- Cropland density (calculated together with landscape metrics)

The raw data is downloaded from [https://download.geofabrik.de/africa.html](https://download.geofabrik.de/africa.html).

#### Waterbodies, rivers, and streams

Waterbbodies and deep rivers should act as barriers for animal movement.

```{r}
library(sf)

# Waterbodies
fname <- list.files(file.path(data_path, "osm"),
                    pattern = "osm_water_a_free_1.shp",
                    full.names = TRUE)
waterbodies <- read_sf(fname)
waterbodies <- waterbodies[
    !waterbodies$fclass %in% c("wetland", "glacier", "dock"), ]

# Rivers
fname <- list.files(file.path(data_path, "osm"),
                    pattern = "osm_waterways_free_1.shp",
                    full.names = TRUE)
rivers <- read_sf(fname)
streams <- rivers[rivers$fclass == "stream", ]
rivers <- rivers[rivers$fclass == "river", ]

# Save out
write_sf(waterbodies, file.path(vct_path, "waterbodies.geojson"))
write_sf(rivers, file.path(vct_path, "rivers.geojson"))
write_sf(streams, file.path(vct_path, "streams.geojson"))

# Remove temporary objects
rm(waterbodies, rivers, streams); gc()
```

#### Roads/railways

```{r}
# Railways
fname <- list.files(file.path(data_path, "osm"),
                    pattern = "osm_railways_free_1.shp",
                    full.names = TRUE)
railways <- read_sf(fname)

# Roads
fname <- list.files(file.path(data_path, "osm"),
                    pattern = "osm_roads_free_1.shp",
                    full.names = TRUE)
roads <- read_sf(fname)

## Remove residential roads
roads <- roads[
    !roads$fclass %in% c("residential", "service", "steps",
                         "pedestrian", "path", "footway", 
                         "cycleway", "living_street"), ]

## Big roads
big_roads <- roads[
    roads$fclass %in% c("trunk", "primary", "trunk_link", "primary_link"), ]
big_roads <- rbind(railways[, c("osm_id", "fclass")], 
                   big_roads[, c("osm_id", "fclass")])

# Save out
write_sf(big_roads, file.path(vct_path, "big_roads.geojson"))
write_sf(roads, file.path(vct_path, "roads.geojson"))

# Remove temporary objects
rm(railways, roads, big_roads); gc()
```

#### Distances

This part is just for the record. We will not use distance-based features for coarse scale analysis. We upscale landcover using area preserving method first to keep the reasonable landscape structure before calculate the distance to cropland.

```{r}
# devtools::install_github("LLeiSong/APUpscale")
library(APUpscale)
library(purrr)

landscape <- rast(file.path(data_path, "landcover/landcover.tif"))
landscape <- upscale(landscape, 
                     cellsize = 1000, 
                     verbose = verbose)
writeRaster(landscape, file.path(data_path, "landcover/landcover_1000m.tif"),
            overwrite = T,
            wopt = list(datatype = 'INT1U',
                        gdal=c("COMPRESS=LZW")))

# Calculate the distances
source(file.path(here("scripts"), "distances.R"))

# Distances
walk(c(10, 5, 2.5), function(scale) {
    message(sprintf("Calculate distance in scale: %s", scale))
    distances(data_path, vct_path, cs_path, scale)
})
```

#### Density
##### Settlement density

Here, we use [Google Open Buildings](https://sites.research.google/open-buildings/) for a better quality from [here](https://sites.research.google/open-buildings/#download).

```{r}
library(sf)
library(terra)
library(pbmcapply)

# Boundary
bry <- read_sf(file.path(vct_path, "mainland_tanzania.geojson"))
bry <- vect(bry["FID"])

# Buildings
fname <- list.files(file.path(data_path, "gob"), full.names = TRUE)

buildings <- read.csv(fname, stringsAsFactors = FALSE)
buildings <- buildings %>% 
  dplyr::select(geometry) %>% 
  dplyr::mutate(geometry = st_as_sfc(geometry)) %>% 
  st_as_sf() %>% st_set_crs(4326) %>% 
  st_make_valid()
buildings <- buildings[!st_is_empty(buildings), , drop = FALSE]

# Save out
write_sf(buildings, file.path(vct_path, "buildings.geojson"))

# calculate the pixel-wise count
# Read buildings and pack it for parallel;
# buildings <- read_sf(file.path(vct_path, "buildings.geojson"))
buildings <- vect(buildings)
buildings <- centroids(buildings, inside = FALSE)
buildings <- wrap(buildings)

# Cut the whole image to tiles
# I guess large vector can slow down the computation significantly
template <- rast(file.path(cs_path, "bios_2.5.tif"),
                 lyrs = 1)
values(template) <- NA
parts <- aggregate(template, fact = 60)
temp_dir <- tempdir()
output_tiles <- makeTiles(
    template, parts,
    filename = file.path(temp_dir, 'output_.tif'))
rm(temp_dir, parts, template)

# Summarize the settlement count within each cell tile by tile
temp_dir <- file.path(tempdir(), "results_settlement")
dir.create(temp_dir)
message('Save raster chunks to %s', temp_dir)

settlement_density <- pbmclapply(output_tiles, function(fname) {
    outname <- file.path(temp_dir, basename(fname))
    rst <- rast(fname)
    vct <- crop(vect(buildings), rst)
    if (nrow(vct) == 0) {
        values(rst) <- 0
        writeRaster(rst, outname)
    } else {
        rst <- rasterizeGeom(vct, rst, fun = "count")
        writeRaster(rst, outname)
    }
    rm(rst, vct); gc()
    outname
}, mc.cores = 6, ignore.interactive = TRUE)
settlement_density <- vrt(unlist(settlement_density))
writeRaster(settlement_density, 
            file.path(cs_path, "settlement_density_2.5.tif"))
rm(buildings, settlement_density)
# unlink(temp_dir, recursive = TRUE)
gc()

# Upscale
walk(c(5, 10), function(scale) {
    template <- rast(file.path(cs_path, sprintf("bios_%s.tif", scale)),
                 lyrs = 1)
    values(template) <- 1:ncell(template)
    zones <- resample(template, settlement_density, method = "near")
    settlement_density_us <- zonal(settlement_density, zones, 
                                   fun = sum)
    values(template) <- settlement_density_us[, 2]
    writeRaster(template, 
                file.path(cs_path, 
                          sprintf("settlement_density_%s.tif", scale)))
})
```

##### Roads density

```{r}
# Read roads and pack it for parallel
roads <- vect(file.path(vct_path, "roads.geojson"))
roads <- roads[, -c(1:ncol(roads))]
roads <- wrap(roads)

# Summarize the road length within each cell tile by tile
temp_dir <- file.path(tempdir(), "results_roads")
dir.create(temp_dir)
message('Save raster chunks to %s', temp_dir)

roads_density <- pbmclapply(output_tiles, function(fname) {
    outname <- file.path(temp_dir, basename(fname))
    rst <- rast(fname)
    vct <- crop(vect(roads), rst)
    if (nrow(vct) == 0) {
        values(rst) <- 0
        writeRaster(rst, outname)
    } else {
        rst <- rasterizeGeom(vct, rst, fun = "length", unit = "km")
        writeRaster(rst, outname)
    }
    outname
}, mc.cores = 6, ignore.interactive = TRUE)

roads_density <- vrt(unlist(roads_density))
writeRaster(roads_density, file.path(cs_path, 
                                     "roads_density_2.5.tif"))
rm(roads, roads_density)
# unlink(temp_dir, recursive = TRUE)
gc()

# upscale
walk(c(5, 10), function(scale) {
    template <- rast(file.path(cs_path, sprintf("bios_%s.tif", scale)),
                 lyrs = 1)
    values(template) <- 1:ncell(template)
    zones <- resample(template, roads_density, method = "near")
    roads_density_us <- zonal(roads_density, zones, fun = sum)
    values(template) <- roads_density_us[, 2]
    writeRaster(template, 
                file.path(cs_path, 
                          sprintf("roads_density_%s.tif", scale)))
})
```

##### River density

```{r}
# Read rivers and streams and pack it for parallel
rivers <- rbind(vect(file.path(vct_path, "rivers.geojson")),
                vect(file.path(vct_path, "streams.geojson")))
rivers <- rivers[, -c(1:ncol(rivers))]
rivers <- wrap(rivers)

# Summarize the river length within each cell tile by tile
temp_dir <- file.path(tempdir(), "results_rivers")
dir.create(temp_dir)
message('Save raster chunks to %s', temp_dir)

rivers_density <- pbmclapply(output_tiles, function(fname) {
    outname <- file.path(temp_dir, basename(fname))
    rst <- rast(fname)
    vct <- crop(vect(rivers), rst)
    if (nrow(vct) == 0) {
        values(rst) <- 0
        writeRaster(rst, outname)
    } else {
        rst <- rasterizeGeom(vct, rst, fun = "length", unit = "km")
        writeRaster(rst, outname)
    }
    outname
}, mc.cores = 6, ignore.interactive = TRUE)

rivers_density <- vrt(unlist(rivers_density))
writeRaster(rivers_density, file.path(cs_path, 
                                      "rivers_density_2.5.tif"))
rm(rivers, rivers_density)
unlink(temp_dir, recursive = TRUE)

# Remove temporary files
file.remove(output_tiles)
rm(rivers, rivers_density, output_tiles, temp_dir)

# upscale
walk(c(5, 10), function(scale) {
    template <- rast(file.path(cs_path, sprintf("bios_%s.tif", scale)),
                 lyrs = 1)
    values(template) <- 1:ncell(template)
    zones <- resample(template, rivers_density, method = "near")
    rivers_density_us <- zonal(rivers_density, zones, fun = sum)
    values(template) <- rivers_density_us[, 2]
    writeRaster(template, 
                file.path(cs_path, 
                          sprintf("rivers_density_%s.tif", scale)))
})
```

### Topographic features

- Terrain roughness (SD of elevation within a cell)
- Slope (SD of slope within a cell)

We used Advanced Land Observing Satellite (ALOS) DSM. DSM is the alternative elevation product of SRTM with approximately 30 meters resolution. 

##### Bounding range of Tanzania

```{r}
# Get the range
bry <- read_sf(file.path(data_path, "vectors/mainland_tanzania.geojson"))
bry <- bry[, "FID"]
ranges <- st_bbox(bry)
ranges[1:2] <- floor(ranges[1:2])
ranges[3:4] <- ceiling(ranges[3:4])
```

##### Target the DSM tiles and download

```{r}
library(stringr)

# Get the tiles
## longitude
if (ranges[1] > 0 & ranges[3] >= 0) {
    message("The whole area is in East side.")
    
    start_loc <- ranges[1] %/% 5 * 5
    end_loc <- (ranges[3] %/% 5 + 1) * 5
    lon_zones <- seq(start_loc, end_loc, 5)
    lon_zones <- paste0("E", str_pad(lon_zones, 3, pad = "0"))
} else if (ranges[1] < 0 & ranges[3] >= 0) {
    message("The whole area is across West and East side.")
    
    start_loc <- ranges[1] %/% 5 * 5
    end_loc <- (ranges[3] %/% 5 + 1) * 5
    lon_zones <- seq(start_loc, end_loc, 5)
    lon_signs <- ifelse(lon_zones >= 0, "E", "W")
    lon_zones <- paste0(lon_signs, str_pad(abs(lon_zones), 3, pad = "0"))
    rm(lon_signs)
} else if (ranges[1] < 0 & ranges[3] < 0) {
    message("The whole area is in West side.")
    
    start_loc <- ranges[1] %/% 5 * 5
    end_loc <- (ranges[3] %/% 5 + 1) * 5
    lon_zones <- seq(start_loc, end_loc, 5)
    lon_zones <- paste0("W", str_pad(abs(lon_zones), 3, pad = "0"))
}

## latitude
if (ranges[2] > 0 & ranges[4] >= 0) {
    message("The whole area is in North side.")
    
    start_loc <- ranges[2] %/% 5 * 5
    end_loc <- (ranges[4] %/% 5 + 1) * 5
    lat_zones <- seq(start_loc, end_loc, 5)
    lat_zones <- paste0("N", str_pad(lat_zones, 3, pad = "0"))
} else if (ranges[2] < 0 & ranges[4] >= 0) {
    message("The whole area is across South and North side.")
    
    start_loc <- ranges[2] %/% 5 * 5
    end_loc <- (ranges[4] %/% 5 + 1) * 5
    lat_zones <- seq(start_loc, end_loc, 5)
    lat_signs <- ifelse(lat_zones >= 0, "N", "S")
    lat_zones <- paste0(lat_signs, str_pad(abs(lat_zones), 3, pad = "0"))
    rm(lat_signs)
} else if (ranges[2] < 0 & ranges[4] < 0) {
    message("The whole area is in South side.")
    
    start_loc <- ranges[2] %/% 5 * 5
    end_loc <- (ranges[4] %/% 5 + 1) * 5
    lat_zones <- seq(start_loc, end_loc, 5)
    lat_zones <- paste0("S", str_pad(abs(lat_zones), 3, pad = "0"))
}; rm(ranges, start_loc, end_loc)

zones <- sapply(1:(length(lat_zones) - 1), function(n) {
    sprintf("%s%s_%s%s", lat_zones[n], 
            lon_zones[1:(length(lon_zones) - 1)],
            lat_zones[n + 1], 
            lon_zones[2:length(lon_zones)])})
base_url <- "https://www.eorc.jaxa.jp/ALOS/aw3d30/data/release_v2012"
urls <- file.path(base_url, sprintf("%s.zip", zones))
rm(zones, base_url, lat_zones, lon_zones)

dst_path <- file.path(data_path, "DSM")
if (!dir.exists(dst_path)) dir.create(dst_path)

opts <- options()
options(timeout = 60 * 120)
lapply(urls, function (url_dl) {
    message(sprintf("Download %s to %s", url_dl, dst_path))
    
    # Download
    fname <- file.path(dst_path, basename(url_dl))
    tryCatch({
        download.file(url_dl, 
                  destfile = fname)
    }, error = function(e) {
        warning("Failed to download, try again.")
        file.remove(fname)
        tryCatch({
            download.file(url_dl, 
                  destfile = fname)
        }, error = function(e) {
            warning("Failed to download again.")
            file.remove(fname)})})
    
    # unzip
    if (file.exists(fname)) {
        cmd <- sprintf("unzip %s -d %s", fname, dst_path)
        try(system(cmd, intern = TRUE, ignore.stdout = TRUE))
        file.remove(fname)}
}); options(opts)
rm(bopts, cmd, data_path, fname, url_dl, urls)
```

##### Mosaic to a single file

```{r}
# Mosaic them together
dsm <- do.call(merge, lapply(
    list.files(dst_path, recursive = TRUE, 
               pattern = "DSM.tif", 
               full.names = TRUE), function(fname) {
                   rast(fname)}))
dsm <- crop(dsm, bry)
writeRaster(dsm, file.path(rst_path, "dsm_tanzania.tif"),
            datatype = "INT4S", overwrite = TRUE,
            wopt = list(gdal=c("COMPRESS=LZW")))
```

#### Topographic roughness

Here, we calculated standard deviation of elevation and slope within each BIO cell.

```{r}
library(raster)
library(purrr)
library(pbmcapply)

dsm <- raster(dsm)
walk(c(10, 5, 2.5), function(scale) {
    message(sprintf("Process scale: %s", scale))
    template <- raster(file.path(cs_path, sprintf("bios_%s.tif", scale)), 
                     band = 1)
    values(template) <- 1:ncell(template)
    vals <- do.call(rbind, pbmclapply(1:ncell(template), function(id) {
        blk <- classify(rast(template), cbind(id, 1), others = NA)
        blk <- trim(blk)
        
        # Do calculation for each window size
        if (relate(as.polygons(ext(dsm)), 
                   as.polygons(ext(blk)), "intersects")) {
            # Crop the landscape
            dsm_blk <- crop(rast(dsm), blk)
            
            # Calculate the metrics
            if (isTRUE(global(dsm_blk, fun = "isNA") != ncell(dsm_blk))) {
                sd_elev <- sd(values(dsm_blk), na.rm = TRUE)
                sd_slope <- sd(values(terrain(dsm_blk, "slope")), na.rm = TRUE)
                data.frame(sd_elevation = sd_elev, sd_slope = sd_slope)
            } else {
                return(NA)
            }
        } else return(NA)
    }, mc.cores = detectCores() - 1))
    
    # Burn values to raster
    roughness <- rep(rast(template), ncol(vals))
    values(roughness) <- as.matrix(vals)
    names(roughness) <- names(vals)
    dst_path <- file.path(cs_path, sprintf("terrain_roughness_%s.tif", scale))
    writeRaster(roughness, dst_path)
})
```

## Modeling in different scales

- 2.5 minute
- 5 minute
- 10 minute

Here, we use Isolation forest method for modeling in this part. Like a regular modeling, we do parameter tuning, select the best model, and run the final model. Differently, we used random sampling within range map as pseudo occurrences for modeling. And we use the average of 10-folds for the final model. We use the true occurrence as an "independent test" to select the best scale.

### Gather data

In this step, we stack all environmental variables for each scale, and fill the potential NAs.

```{r}
source(file.path(here("scripts"), "gather_data.R"))

# boundary
bry <- read_sf(file.path(data_path, "vectors/mainland_tanzania.geojson"))
bry <- vect(bry[, 'FID'])

walk(c(2.5, 5, 10), function(scale) {
    gather_data_cs(bry, cs_path, scale)
})
```

### Variable analysis

Due to the high correlation between landscape metrics, we do pair-wise correlation check again. And remove the variables that are highly correlated with others. We keep some highly-correlated ones, such as dry season NDVI and wet season NDVI because we think both of them are ecological meaningful.

```{r}
# Check correlation and reduce dimension
walk(c(2.5, 5, 10), function(scale) {
    vars <- rast(file.path(cs_path, sprintf("variables_%s.tif", scale)))
    vars <- values(vars)
    vars <- data.frame(vars) %>% na.omit()
    vars <- vars %>% 
        select(-c(sd_slope, ai, sidi, condent, 
                  ent, joinent, mutinf, ed, savanna_lpi,
                  cropland_ratio, tree_ratio))
    corrplot(corr = cor(vars, method = "spearman"), 
             method = "square", type = "lower",
             diag = FALSE, addCoef.col = "black", 
             number.cex = 0.7, tl.cex = 0.7)
})

# Save out the selected variables
walk(c(2.5, 5, 10), function(scale) {
    vars <- rast(file.path(cs_path, sprintf("variables_%s.tif", scale)))
    bands_select <- setdiff(names(vars), 
                            c('sd_slope', 'ai', 'sidi', 'condent', 
                              'ent', 'joinent', 'mutinf', 'ed', 'savanna_lpi',
                              'cropland_ratio', 'tree_ratio'))
    vars <- subset(vars, bands_select)
    writeRaster(vars, 
                file.path(cs_path, 
                          sprintf("variables_final_%s.tif", scale)))
})
```

So the final variables to use are:

**Bioclimatic variables:**

- BIO1 = Annual Mean Temperature
- BIO4 = Temperature Seasonality (standard deviation ×100)
- BIO7 = Temperature Annual Range (BIO5-BIO6)
- BIO12 = Annual Precipitation
- BIO15 = Precipitation Seasonality (Coefficient of Variation)

**Vegetation:**

- Mean NDVI during wet season
- Mean NDVI during dry season
- NDVI Seasonality (standard deviation)

**Density-based features:**

- Settlement density
- Roads density
- River density

**Topography:**

- SD of elevation

**Landscape condition:**

Landscape level:

- Patch density (PD)
- Patch richness density (PRD)
- Shannon’s diversity index (SHDI)
- Ratio of Savanna (Shrub, grassland, and wetland)
- Ratio of Water

Class level:

- Edge density (ED) of cropland
- Edge density (ED) of tree
- Contiguity index distribution (mean) (CONTIG_MN) of savanna
- Patch density (PD) of savanna

### Modeling with Isolation Forest

Isolation forest works like an advanced version of bioclim. The direct result can be interpreted as environmental niche. So we use this method at coarse scale in this part even though the test performance may be slightly lower. This makes sense because the result is environmental niche rather than probability of occurrence and elephant is a common species in Tanzania.

#### Hyper-parameter tuning

Here, we first tune the model parameters according to others' suggestions:

- sample_size = c(0.8, 0.9, 1.0)
- max_depth = c(20, 25, 30)
- ndim = c(2, 3, 4)
- scoring_metric = c("depth", "adj_depth")

```{r}
result_path <- here("results")
source(here("scripts/modeling.R"))

census_block <- read_sf(
    file.path(data_path, "observations/pseudo_blocks.geojson")) %>% 
    filter(density > 0)

walk(c(10, 5, 2.5), function(scale) {
    param_tuning_cs(census_block, scale, cs_path, result_path)
})
```

#### Simulation

```{r}
occ <- read_sf(file.path(data_path, "observations/ele_occurrence.geojson"))

walk(c(10, 5, 2.5), function(scale) {
    load(file.path(result_path, sprintf("tuning_cv_%s.rda", scale)))
    # Get the best parameters for different assessment metrics
    # Pick auc, auc_ratio, CBI, TSS and f-measure as the assessment
    # to pick the best params
    best_params <- tuning_cv %>% 
        arrange(desc(TSS), desc(auc_ratio), desc(auc), desc(`f-measure`)) %>%
        slice(1) %>% 
        select(sample_size, max_depth, ndim, scoring_metric)
    
    # run models
    modeling_cs(best_params, census_block, occ, scale, cs_path, result_path)
})
```

#### Gather results

So, expected results from this part are the important variables, how elephants correspond to important variables, and the best scale in 2.5, 5, and 10 minutes.

Then remove the not important and not responsive variables and use the best scale to make the final map at coarse scale. This result will be used as prior probability in the next part.

##### Model evaluation

Both train and test evaluation are important and should be considered in this step.

```{r}

```

##### Variable importance

```{r}

```

##### Variable corresponses

```{r}

```

Predicted environmental suitability at best coarse scales

```{r}
walk(c(10, 5, 2.5), function(scale) {
    load(file.path(result_path, sprintf("runs_%s.rda", scale)))
    prediction <- merge(do.call(c, lapply(runs, function(run) {
        run$prediction
    }))) %>% st_apply(c("x", "y"), mean, na.rm = TRUE)
    
    fname <- file.path(result_path, sprintf("landscape_utility_%s.tif", scale))
    write_stars(prediction, fname)
})
```

### Modeling with Maxent

```{r}
# Test for Maxent
## Prepare data
source(here("scripts/gather_data.R"))
census_block <- read_sf(
    file.path(data_path, "observations/pseudo_blocks.geojson")) %>% 
    filter(density > 0)
occ <- read_sf(file.path(data_path, "observations/ele_occurrence.geojson"))
dst_path <- file.path(data_path, "maxent")

walk(c(10, 5, 2.5), function(scale) {
    gather_data_maxent(census_block, occ, scale, cs_path, dst_path)
})

## Do modeling in Maxent binary
## Auto features (Linear, Quadratic, Product, Hige features)
## Create response curves, Do jackknife to measure variable importance
## Output: Cloglog
## Settings: 
### Rgularization multiplier 2
### Max number of background points: 2000
### Subsampling for 10 times
### Use the spatial thinned real occurrence as independent test
```

